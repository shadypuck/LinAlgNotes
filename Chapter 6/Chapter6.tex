\documentclass{article}

\input{../preamble.tex}

\begin{document}




\lhead{Chapter 6: Eigenvalues}
\section*{Introduction to Eigenvalues and Eigenvectors}
\begin{itemize}
    \item \marginnote{1/28:}[-8.5mm]$Ax=b=\lambda x$
    \item $Ax=\lambda x$, $\lambda\in\mathbb{F}$, $x\in\mathbb{R}^n$
    \item $\lambda$ is an eigenvalue. $\lambda x$ is an eigenvector.
    \begin{equation*}
        A=
        \begin{bmatrix}
            3 & 1\\
            1 & 3\\
        \end{bmatrix}
    \end{equation*}
    \item $
        x =
        \begin{bmatrix}
            1\\
            1\\
        \end{bmatrix}
    $ is an eigenvector of $A$ with corresponding eigenvalue of $4$.
    \item $
        \begin{bmatrix}
            3 & 1\\
            1 & 3\\
        \end{bmatrix}
        \begin{bmatrix}
            1\\
            1\\
        \end{bmatrix}
        =
        \begin{bmatrix}
            4\\
            4\\
        \end{bmatrix}
        = 4
        \begin{bmatrix}
            1\\
            1\\
        \end{bmatrix}
    $
    \begin{center}
        \begin{tikzpicture}
            \draw (-2,0) -- (2,0);
            \draw (0,-2) -- (0,2);
            \draw [thick,->] (0,0) -- node[right]{$x$} (0.5,0.5);
            \draw [thick,dashed,->] (0,0) -- node[right]{$Ax$} (2,2);
        \end{tikzpicture}
    \end{center}
    \begin{align*}
        Ax &= \lambda x\\
        Ax-\lambda x &= 0\\
        Ax-\lambda Ix &= 0\\
        (A-\lambda I)x &= 0
    \end{align*}
    \item $(A-\lambda I)x=0 \Rightarrow x\in N(A-\lambda I)\footnote{To have a null space, $A-\lambda I$ has free columns.}\Rightarrow |A-\lambda I|=0$
    \item $
        \begin{bmatrix}
            3 & 1\\
            1 & 3\\
        \end{bmatrix}
        -
        \begin{bmatrix}
            \lambda & 0\\
            0 & \lambda\\
        \end{bmatrix}
        =
        \begin{bmatrix}
            3-\lambda & 1\\
            1 & 3-\lambda\\
        \end{bmatrix}
    $
    \item $
        \begin{vmatrix}
            3-\lambda & 1\\
            1 & 3-\lambda\\
        \end{vmatrix}
        = 0
    $
    \begin{align*}
        0 &= (3-\lambda)^2-1^2\\
        &= 3^2-6\lambda+\lambda^2-1\\
        &= \lambda^2-6\lambda+8\\
        &= (\lambda-4)(\lambda-2)
    \end{align*}
    \item $\lambda=4,2$.
    \item $\lambda^2-6\lambda+8$ is the \textbf{characteristic polynomial} of $A$.
    \item $
        A-2I =
        \begin{bmatrix}
            -1 & 1\\
            1 & -1\\
        \end{bmatrix}
        ,\ 
        x =
        \begin{bmatrix}
            1\\
            -1\\
        \end{bmatrix}
        \in N(A-2I)
    $.
    \item $
        A-4I =
        \begin{bmatrix}
            -1 & 1\\
            1 & -1\\
        \end{bmatrix}
        ,\ 
        x =
        \begin{bmatrix}
            1\\
            1\\
        \end{bmatrix}
        \in N(A-4I)
    $.
    \item "Eigenspace" is not $\mathbb{R}^2$, but two lines in $\mathbb{R}^2$, specifically $y=\pm x$.
    \begin{itemize}
        \item $y=\pm x$ comes from $c_1\begin{bmatrix}1\\1\end{bmatrix}$ and $c_2\begin{bmatrix}1\\-1\end{bmatrix}$.
    \end{itemize}

    %%%%%%%%%%%%%%%%%%%%%%

    \marginnote{1/29:}\begin{equation*}
        A =
        \begin{bmatrix}
            2 & -2 & 3\\
            0 & 3 & -2\\
            0 & -1 & 2\\
        \end{bmatrix}
    \end{equation*}
    \begin{align*}
        P(\lambda) &= |A-\lambda I|\\
        &=
        \begin{vmatrix}
            2-\lambda & -2 & 3\\
            0 & 3-\lambda & -2\\
            0 & -1 & 2-\lambda\\
        \end{vmatrix}\\
        &= -1
        \begin{vmatrix}
            2-\lambda & 3\\
            0 & -2\\
        \end{vmatrix}
        (-1)^{3+2} + (2-\lambda)
        \begin{vmatrix}
            2-\lambda & -2\\
            0 & 3-\lambda\\
        \end{vmatrix}
        (-1)^{3+3}\\
        &= ((2-\lambda)(-2))+(2-\lambda)((2-\lambda)(3-\lambda))\\
        &= -4+2\lambda+(2-\lambda)^2(3-\lambda)\\
        &= -4+2\lambda+(4-4\lambda+\lambda^2)(3-\lambda)\\
        &= -4+2\lambda+12-4\lambda-12\lambda+4\lambda^2+3\lambda^2-\lambda^3\\
        &= -\lambda^3+7\lambda^2-14\lambda+8\\
        &= -(\lambda-1)(\lambda-2)(\lambda-4)
    \end{align*}
    \begin{align*}
        A-I &=
        \begin{bmatrix}
            1 & -2 & 3\\
            0 & 2 & -2\\
            0 & -1 & 1\\
        \end{bmatrix}&
        A-2I &=
        \begin{bmatrix}
            0 & -2 & 3\\
            0 & 1 & -2\\
            0 & -1 & 0\\
        \end{bmatrix}&
        A-4I &=
        \begin{bmatrix}
            -2 & -2 & 3\\
            0 & -1 & -2\\
            0 & -1 & -2\\
        \end{bmatrix}&
    \end{align*}
    \item $P(\lambda)$ is positive when $n\in 2\mathbb{N}$, negative otherwise.
    \begin{itemize}
        \item Signs flip term to term (think about binomial expansion).
    \end{itemize}
    \item Coefficient of the $n-1$ degree term is the sum of the diagonal entries.
    \item Coefficient of the $0^\text{th}$ degree term is $|A|$.
    \begin{itemize}
        \item $P_\lambda(0) = |A-0\cdot I| = |A|$.
    \end{itemize}
    \item Product of the eigenvalues is $|A|$.
    \begin{itemize}
        \item Think about expanding the factorization of $P(\lambda)$.
    \end{itemize}
    \item Eigenvalues of $U$ are the diagonal values.
    \begin{itemize}
        \item $\lambda_1\lambda_2\cdots\lambda_n=|A|$, which is the product of the diagonal entries.
        \item $\lambda_1+\cdots+\lambda_n=\text{trace}(A)$, which is the sum of the diagonal entries.
    \end{itemize}
    \item $Ax=\lambda x$
    \begin{itemize}
        \item $A^2x=AAx=A\lambda x=\lambda Ax=\lambda\lambda x=\lambda^2x$
    \end{itemize}
\end{itemize}



\section*{Similarity}
\begin{itemize}
    \item \marginnote{1/30:}$A\sim B$\footnote{$A$ "is similar to" $B$} iff $\exists\ S:A=SBS^{-1},\ B=S^{-1}AS$.
    \begin{enumerate}
        \item If $A\sim B$, then $|A|=|B|$.
        \begin{align*}
            B &= S^{-1}AS\\
            |B| &= |S^{-1}AS|\\
            |B| &= |S^{-1}||A||S|\\
            |B| &= \frac{1}{|S|}|A||S|\\
            |B| &= |A|
        \end{align*}
        \item If $A\sim B$, then they share the same characteristic polynomial.
        \begin{align*}
            B &= S^{-1}AS\\
            |B-\lambda I| &= |S^{-1}AS-\lambda I|\\
            &= |S^{-1}AS-\lambda S^{-1}IS|\\
            &= |S^{-1}(A-\lambda I)S|\\
            &= \frac{1}{|S|}|A-\lambda I||S|\\
            |B-\lambda I| &= |A-\lambda I|
        \end{align*}
        \begin{itemize}
            \item If they have the same characteristic polynomial, $\therefore$ $A$ and $B$ have the same eigenvalues.
        \end{itemize}
    \end{enumerate}
    \item What is the best possible $B$ if $A\sim B$?
    \begin{itemize}
        \item Sparse.
        \item Diagonal.
        \item $
            A=[\text{ugly}]\quad\rightarrow\quad
            B=\begin{bmatrix}
                \lambda_1 &  & 0\\
                 & \ddots & \\
                0 &  & \lambda_n\\
            \end{bmatrix}
            =\Lambda
        $
    \end{itemize}
    \item \textbf{Diagonalization}:
    \begin{align*}
        A &= S\Lambda S^{-1}\\
        AS &= S\Lambda\\
        \Lambda &= S^{-1}AS
    \end{align*}
    \item $A=S\Lambda S^{-1}$
    \begin{itemize}
        \item $A^2 = AA = S\Lambda S^{-1}S\Lambda S^{-1} = S\Lambda\Lambda S^{-1} = S\Lambda^2 S^{-1}$
        \item $A^k = S\Lambda^k S^{-1}$
        \item $
            A^k = S
            \begin{bmatrix}
                {\lambda_1}^k &  & 0\\
                 & \ddots & \\
                0 &  & {\lambda_n}^k\\
            \end{bmatrix}
            S^{-1}
        $
    \end{itemize}
    \item Diagonalize the following matrix $A$.
    \begin{equation*}
        A =
        \begin{bmatrix}
            -1 & 0 & 1\\
            3 & 0 & -3\\
            1 & 0 & -1\\
        \end{bmatrix}
    \end{equation*}
    \begin{itemize}
        \item Find the characteristic polynomial.
        \begin{align*}
            |A-\lambda I| &=
            \begin{vmatrix}
                -1-\lambda & 0 & 1\\
                3 & 0-\lambda & -3\\
                1 & 0 & -1-\lambda\\
            \end{vmatrix}\\
            &= (-1-\lambda)((-\lambda)(-1-\lambda))+(-1)(-\lambda)\\
            &= -\lambda(-1-\lambda)^2+\lambda\\
            &= -\lambda(1+2\lambda+\lambda^2)+\lambda\\
            &= -\lambda^3-2\lambda^2\\
            &= -\lambda^2(\lambda+2)
        \end{align*}
        \item Find the eigenvalues: $\lambda_1=\lambda_2=0$, $\lambda_3=-2$
        \item \textbf{Algebraic multiplicity} of $\lambda_1,\lambda_2$ is 2.
        \item A.M. of $\lambda_3$ is 1.
        \item $
            A-0I =
            \begin{bmatrix}
                -1 & 0 & 1\\
                3 & 0 & -3\\
                1 & 0 & -1\\
            \end{bmatrix}
        $
        \item $\text{rank}(A-0I)=1 \Rightarrow \dim(N(A-0I))=2$
        \item The 2 directly above is the \textbf{geometric multiplicity}.
        \item $A$ is diagonalizable iff A.M. of $\lambda_i=\text{G.M.}$
        
        %%%%%%%%%%%%%%%%%%%%%%%%%
        
        \item \marginnote{1/31:}Eigenvectors are $
            x_1=
            \begin{bmatrix}
                1\\
                0\\
                1\\
            \end{bmatrix}
        $ and $
            x_2=
            \begin{bmatrix}
                0\\
                1\\
                0\\
            \end{bmatrix}
        $.
        \item $
        A+2I =
        \begin{bmatrix}
            1 & 0 & 1\\
            3 & 2 & -3\\
            1 & 0 & 1\\
        \end{bmatrix}
    $
        \item Eigenvector is $
            x_3=
            \begin{bmatrix}
                1\\
                -3\\
                -1\\
            \end{bmatrix}
        $.
        \item Use an $S$ matrix of eigenvectors.
        \item $
            A = S\Lambda S^{-1} = \frac{1}{2}
            \begin{bmatrix}
                0 & 1 & 1\\
                1 & 0 & -3\\
                0 & 1 & -1\\
            \end{bmatrix}
            \begin{bmatrix}
                0 &  & \\
                 & 0 & \\
                 &  & -2\\
            \end{bmatrix}
            \begin{bmatrix}
                3 & 2 & -3\\
                1 & 0 & 1\\
                1 & 0 & -1\\
            \end{bmatrix}
        $
        \item Note that $
            A^{9752} = \frac{1}{2}
            \begin{bmatrix}
                0 & 1 & 1\\
                1 & 0 & -3\\
                0 & 1 & -1\\
            \end{bmatrix}
            \begin{bmatrix}
                0 &  & \\
                 & 0 & \\
                 &  & (-2)^{9752}\\
            \end{bmatrix}
            \begin{bmatrix}
                3 & 2 & -3\\
                1 & 0 & 1\\
                1 & 0 & -1\\
            \end{bmatrix}
        $
    \end{itemize}
    \item \textbf{Algebraic multiplicity}: The number of repeated roots to a polynomial. For all of the roots, it adds up to $n$ ($n$-square matrix). \emph{Also known as} \textbf{A.M.}
    \item \textbf{Geometric multiplicity}: The number of eigenvectors produced from each root. For all of the roots, it may not add up to $n$ ($n$-square matrix). $\dim(N(A-\lambda I))$. \emph{Also known as} \textbf{G.M.}
    \item A nondiagonalizable example:
    \begin{equation*}
        A =
        \begin{bmatrix}
            1 & 1 & 0\\
            0 & 1 & 1\\
            0 & 0 & 4\\
        \end{bmatrix}
    \end{equation*}
    \begin{itemize}
        \item $\lambda_1 = \lambda_2 = 1$ and $\lambda_3 = 4$.
        \item $\lambda_1$ and $\lambda_2$ have $\text{A.M.}={\color{red}2}$.
        \item $\lambda_3$ has $\text{A.M.}=1$.
        \item $
            A-I=
            \begin{bmatrix}
                0 & 1 & 0\\
                0 & 0 & 1\\
                0 & 0 & 3\\
            \end{bmatrix}
        $
        \item $\text{rank}(A-I)=2\Rightarrow\dim(N(A-I))=1\Rightarrow\text{G.M.}={\color{red}1}$.
        \item $
            x_1=
            \begin{bmatrix}
                1\\
                0\\
                0\\
            \end{bmatrix}
        $
        \item $
            A-4I=
            \begin{bmatrix}
                -3 & 1 & 0\\
                0 & -3 & 1\\
                0 & 0 & 0\\
            \end{bmatrix}
        $
        \item $
            x_2=
            \begin{bmatrix}
                1\\
                3\\
                9\\
            \end{bmatrix}
        $
        \item $S$ would be $3\times 2$ and, thus, not square, so $\nexists\ S^{-1}$\footnote{At a later date, we will look at an analogy of projections to diagonalization that finds the "best possible" diagonalization (which may not be perfectly diagonal).}.
    \end{itemize}
    \item \textbf{Canonical} (form): An accepted way of expressing something.
\end{itemize}



\section*{Markov Chains}
\marginnote{2/3:}
\begin{center}
    \begin{tikzpicture}
        \node (pgh) [circle,draw] {PGH}
            edge [out=100,in=130,loop,thick] node[above]{0.95} ()
        ;
        \node (cle) [circle,draw] at (5,0) {CLE}
            edge [out=80,in=50,loop,thick] node[above]{0.8} ()
        ;
        \draw [->,thick] (pgh) to[bend left=30] node[above]{0.05} (cle);
        \draw [->,thick] (cle) to[bend left=30] node[below]{0.2} (pgh);
    \end{tikzpicture}
\end{center}
\begin{itemize}
    \item $
        u_0 =
        \begin{bmatrix}
            500,000\\
            500,000\\
        \end{bmatrix}
    $
    \item $Au_0 = u_1$.
    \item $Au_1 = u_2$, $A(Au_0) = u_2$, $A^2u_0 = u_2$, $A^ku_0 = u_k$, $(S\Lambda S^{-1})^ku_0 = u_k$, $S\Lambda^kS^{-1}u_0 = u_k$.
    \begin{align*}
        A &=
        \begin{bmatrix}
            0.95 & 0.20\\
            0.05 & 0.80\\
        \end{bmatrix}&
        u_k &=
        \begin{bmatrix}
            \text{PGH}\\
            \text{CLE}\\
        \end{bmatrix}
    \end{align*}
    \item $A$ is a \textbf{Markov matrix}, where all columns add to 1.
    \item $
        Au_0 =
        \begin{bmatrix}
            0.95 & 0.20\\
            0.05 & 0.80\\
        \end{bmatrix}
        \begin{bmatrix}
            500,000\\
            500,000\\
        \end{bmatrix}
        = u_1
    $
    \begin{align*}
        |A-\lambda I| &=
        \begin{vmatrix}
            0.95-\lambda & 0.20\\
            0.05 & 0.80-\lambda\\
        \end{vmatrix}\\
        &= (0.95-\lambda)(0.8-\lambda)-(0.2)(0.05)\\
        &= (\lambda-1)(\lambda-0.75)
    \end{align*}
    \item $\lambda_1 = 1,\ \lambda_2 = 0.75$.
    \item $
        A-I =
        \begin{bmatrix}
            -0.05 & 0.2\\
            0.05 & -0.2\\
        \end{bmatrix}
        \Rightarrow
        x =
        \begin{bmatrix}
            4\\
            1\\
        \end{bmatrix}
    $
    \item $
        A-0.75I =
        \begin{bmatrix}
            0.2 & 0.2\\
            0.05 & 0.05\\
        \end{bmatrix}
        \Rightarrow
        x =
        \begin{bmatrix}
            -1\\
            1\\
        \end{bmatrix}
    $
    \begin{align*}
        A^ku_0 &= \frac{1}{5}
        \begin{bmatrix}
            4 & -1\\
            1 & 1\\
        \end{bmatrix}
        \begin{bmatrix}
            1 & 0\\
            0 & 0.75\\
        \end{bmatrix}^k
        \begin{bmatrix}
            1 & 1\\
            -1 & 4\\
        \end{bmatrix}
        \begin{bmatrix}
            500,000\\
            500,000\\
        \end{bmatrix}\\
        &=
        \begin{bmatrix}
            4 & -1\\
            1 & 1\\
        \end{bmatrix}
        \begin{bmatrix}
            1 & 0\\
            0 & 0.75^k\\
        \end{bmatrix}
        \begin{bmatrix}
            200,000\\
            300,000\\
        \end{bmatrix}\\
        &=
        \begin{bmatrix}
            4\\
            1\\
        \end{bmatrix}
        (200,000) +
        \begin{bmatrix}
            -1\\
            1\\
        \end{bmatrix}
        (0.75)^k(300,000)
    \end{align*}
    \item $
        \begin{bmatrix}
            800,000\\
            200,000\\
        \end{bmatrix}
    $ is the steady-state vector.
    \item $
        \begin{bmatrix}
            -(0.75)^k(300,000)\\
            (0.75)^k(300,000)\\
        \end{bmatrix}
    $ is the dynamically changing vector.
    \item $
        \lim_{k\to\infty} A^ku_0 =
        \begin{bmatrix}
            800,000\\
            200,000\\
        \end{bmatrix}
        =
        \begin{bmatrix}
            \text{PGH}\\
            \text{CLE}\\
        \end{bmatrix}
    $
\end{itemize}


\subsection*{Explicit Formula for the Fibonacci Sequence}
\begin{itemize}
    \item \marginnote{2/4:}1, 1, 2, 3, 5, 8, \dots
    \item Recursively defined formula: $F_n\footnote{The $n$-th Fibonacci number.}=F_{n-1}+F_{n-2}$.
    \begin{align*}
        F_n &= F_{n-1}+F_{n-2}\\
        F_{n-1} &= F_{n-1}\\
        \begin{bmatrix}
            F_n\\
            F_{n-1}\\
        \end{bmatrix}
        &=
        \begin{bmatrix}
            1 & 1\\
            1 & 0\\
        \end{bmatrix}
        \begin{bmatrix}
            F_{n-1}\\
            F_{n-2}\\
        \end{bmatrix}
    \end{align*}
    \item $u_n = A^nu_0 = S\Lambda^nS^{-1}u_0$.
    \begin{align*}
        0 &= |A-\lambda I|\\
        &=
        \begin{vmatrix}
            1-\lambda & 1\\
            1 & -\lambda\\
        \end{vmatrix}\\
        &= -\lambda(1-\lambda)-1\\
        &= \lambda^2-\lambda-1
    \end{align*}
    \item $\lambda = \frac{1\pm\sqrt{5}}{2}$\footnote{This is the Golden ratio!}.
    \item $\lambda_1 = \frac{1+\sqrt{5}}{2}$.
    \begin{align*}
        N(A-\lambda_1I) &= N\left(
        \begin{bmatrix}
            1-\frac{1+\sqrt{5}}{2} & 1\\
            1 & -\frac{1+\sqrt{5}}{2}\\
        \end{bmatrix}
        \right)\\
        &= N\left(
        \begin{bmatrix}
            \frac{1-\sqrt{5}}{2} & 1\\
            1 & \frac{-1-\sqrt{5}}{2}\\
        \end{bmatrix}
        \right)\\
    \end{align*}
    \item $
        \begin{bmatrix}
            \frac{1-\sqrt{5}}{2} & 1\\
            1 & \frac{-1-\sqrt{5}}{2}\\
        \end{bmatrix}
        \begin{bmatrix}
            x_1\\
            x_2\\
        \end{bmatrix}
        =
        \begin{bmatrix}
            0\\
            0\\
        \end{bmatrix}
    $
    \item Let $x_2=1$.
    \begin{align*}
        \frac{1-\sqrt{5}}{2}x_1+1 &= 0\\
        \frac{1-\sqrt{5}}{2}x_1 &= -\frac{2}{2}\\
        x_1 &= \frac{-2}{1-\sqrt{5}}\times \frac{1+\sqrt{5}}{1+\sqrt{5}}\\
        &= \frac{-2-2\sqrt{5}}{-4}\\
        &= \frac{1+\sqrt{5}}{2}
    \end{align*}
    \item $
        s_1 =
        \begin{bmatrix}
            \frac{1+\sqrt{5}}{2}\\
            1\\
        \end{bmatrix}
    $, $
        s_2 =
        \begin{bmatrix}
            \frac{1-\sqrt{5}}{2}\\
            1\\
        \end{bmatrix}
    $.
    \item $
        \begin{bmatrix}
            1 & 1\\
            1 & 0\\
        \end{bmatrix}
        =
        \begin{bmatrix}
            \frac{1+\sqrt{5}}{2} & \frac{1-\sqrt{5}}{2}\\
            1 & 1\\
        \end{bmatrix}
        \begin{bmatrix}
            \frac{1+\sqrt{5}}{2} & 0\\
            0 & \frac{1-\sqrt{5}}{2}\\
        \end{bmatrix}
        S^{-1}
    $
    \item $
        S^{-1}
        = \frac{1}{|S|}C_S^\T
        = \frac{1}{\sqrt{5}}
        \begin{bmatrix}
            1 & \frac{-1+\sqrt{5}}{2}\\
            -1 & \frac{1+\sqrt{5}}{2}\\
        \end{bmatrix}
    $
    \item $u_k = A^ku_0 = S\Lambda^kS^{-1}u_0$.
    \begin{align*}
        S^{-1}u_0 &= \frac{1}{\sqrt{5}}
        \begin{bmatrix}
            1 & \frac{-1+\sqrt{5}}{2}\\
            -1 & \frac{1+\sqrt{5}}{2}\\
        \end{bmatrix}
        \begin{bmatrix}
            1\\
            1\\
        \end{bmatrix}\\
        &= \frac{1}{\sqrt{5}}
        \begin{bmatrix}
            \frac{1+\sqrt{5}}{2}\\
            \frac{-1+\sqrt{5}}{2}\\
        \end{bmatrix}\\
        &=
        \begin{bmatrix}
            \frac{1+\sqrt{5}}{2\sqrt{5}}\\
            \frac{-1+\sqrt{5}}{2\sqrt{5}}\\
        \end{bmatrix}\\
        &=
        \begin{bmatrix}
            \frac{5+\sqrt{5}}{10}\\
            \frac{5-\sqrt{5}}{10}\\
        \end{bmatrix}
    \end{align*}
    \item $
        u_k =
        \begin{bmatrix}
            \frac{1+\sqrt{5}}{2}\\
            1\\
        \end{bmatrix}
        \left( \frac{1+\sqrt{5}}{2} \right)^k\left( \frac{5+\sqrt{5}}{10} \right)+
        \begin{bmatrix}
            \frac{1-\sqrt{5}}{2}\\
            1\\
        \end{bmatrix}
        \left( \frac{1-\sqrt{5}}{2} \right)^k\left( \frac{5-\sqrt{5}}{10} \right)
    $
\end{itemize}



\section*{Systems of First-Order Ordinary Differential Equations}
\begin{itemize}
    \item \marginnote{2/11:}Let $f(x)=y$ and $a,c,K\in\mathbb{F}$.
    \begin{align*}
        \dd[y]{x} &= ay\\
        \frac{1}{y}\dd[y]{x} &= a\\
        \frac{1}{y}\dd[y]{x}\, \dx &= a\, \dx\\
        \frac{1}{y}\, \dy &= a\, \dx\\
        \int \frac{1}{y}\, \dy &= \int a\, \dx\\
        \ln y &= ax+c\\
        y &= \e^{ax+c}\\
        &= \e^{ax}\text{e}^c\\
        &= K\e^{ax}
    \end{align*}
    \item Let $\dd[y]{x}=y'$.
    \begin{itemize}
        \item $y_1' = a_{11}y_1+a_{12}y_2+\cdots+a_{1n}y_n$.
        \item $y_2' = a_{21}y_1+a_{22}y_2+\cdots+a_{2n}y_n$.
        \item $\vdots$
        \item $y_n' = a_{n1}y_1+a_{n2}y_2+\cdots+a_{nn}y_n$.
    \end{itemize}
    \item This is a \textbf{square system} of equations.
    \item Rewrite as $y' = Ay$.
    \begin{equation*}
        \begin{bmatrix}
            y_1'\\
            y_2'\\
            \vdots\\
            y_n'\\
        \end{bmatrix}
        =
        \begin{bmatrix}
            a_{11} & a_{12} & \cdots & a_{1n}\\
            a_{21} & a_{22} & \cdots & a_{2n}\\
            \vdots &        & \ddots &       \\
            a_{n1} & a_{n2} & \cdots & a_{nn}\\
        \end{bmatrix}
        \begin{bmatrix}
            y_1\\
            y_2\\
            \vdots\\
            y_n\\
        \end{bmatrix}
    \end{equation*}
    \item Solve the following system of differential equations.
    \begin{align*}
        y_1' &= 3y_1\\
        y_2' &= -2y_2\\
        y_3' &= 5y_3
    \end{align*}
    \begin{equation*}
        \begin{bmatrix}
            y_1'\\
            y_2'\\
            y_3'\\
        \end{bmatrix}
        =
        \begin{bmatrix}
            3 & 0 & 0\\
            0 & -2 & 0\\
            0 & 0 & 5\\
        \end{bmatrix}
        \begin{bmatrix}
            y_1\\
            y_2\\
            y_3\\
        \end{bmatrix}
    \end{equation*}
    \begin{itemize}
        \item General Solution:
    \end{itemize}
    \begin{align*}
        y_1 &= k_1\e^{3x}\\
        y_2 &= k_2\e^{-2x}\\
        y_3 &= k_3\e^{5x}
    \end{align*}
    \begin{itemize}
        \item Particular Solution (where $y_1(0)=2$, $y_2(0)=-1$, and $y_3(0)=7$ are the initial conditions):
    \end{itemize}
    \begin{align*}
        y_1 &= 2\e^{3x}\\
        y_2 &= -\e^{-2x}\\
        y_3 &= 7\e^{5x}
    \end{align*}
    \item Consider a different system. Remember throughout that we are solving for $y$.
    \begin{align*}
        y_1' &= y_1+y_2\\
        y_2' &= 4y_1-2y_2
    \end{align*}
    \begin{itemize}
        \item The previous system was so easy to solve because the matrix was diagonal. This one (as follows) will not be. Therefore, we should diagonalize it.
    \end{itemize}
    \begin{equation*}
        \begin{bmatrix}
            y_1'\\
            y_2'\\
        \end{bmatrix}
        =
        \begin{bmatrix}
            1 & 1\\
            4 & -2\\
        \end{bmatrix}
        \begin{bmatrix}
            y_1\\
            y_2\\
        \end{bmatrix}
    \end{equation*}
    \begin{itemize}
        \item Start with $y'=Ay$.
        \item Substitute $y=Su$.
        \begin{itemize}
            \item Note that $y=Su\Rightarrow y'=Su'$\footnote{Think about differentiating both sides: $y\rightarrow y'$ is obvious, $S$ will be unchanged because it's just coefficients, and the functions of $u$ will be differentiated.}.
            \item If we can find $u'$ in terms of a diagonal matrix and $u$, we can solve for $y$.
        \end{itemize}
        \begin{center}
            \begin{tikzpicture}
                \draw [red] (-2,0) -- (2,0) node[right]{
                    $
                        \begin{bmatrix}
                            1\\
                            0\\
                        \end{bmatrix}
                    $
                };
                \draw [red] (0,-2) -- (0,2) node[above]{
                    $
                        \begin{bmatrix}
                            0\\
                            1\\
                        \end{bmatrix}
                    $
                };

                \draw [blue] (-0.5,2) node[above left]{
                    $
                        S_2\
                        \begin{bmatrix}
                            -1\\
                            4\\
                        \end{bmatrix}
                    $
                } -- (0.5,-2);
                \draw [blue] (-2,-2) -- (2,2) node[right]{
                    $
                        S_1\
                        \begin{bmatrix}
                            1\\
                            1\\
                        \end{bmatrix}
                    $
                };

                \draw [thick] (-1,2) node[below left]{
                    $\color{red}y_1$
                } node[below=3mm]{
                    $\color{blue}u_1$
                } to[out=-45,in=180] (1.8,1.3);
                \draw [thick] plot[smooth,tension=1.2] coordinates{
                    (-1,-1) (-0.5,-0.3) (0.2,-0.6) (1,0.4)
                };
                \node at (-0.8,-1.2) {
                    $\color{red}y_2$
                };
                \node at (-0.5,-0.9) {
                    $\color{blue}u_2$
                };
            \end{tikzpicture}
        \end{center}
        \item We seek to find a new basis $S$ such that the matrix scaling $u$ will be diagonal.
    \end{itemize}
    \begin{align*}
        Su' &= Ay\\
        Su' &= ASu\\
        u' &= S^{-1}ASu\\
        u' &= \Lambda u
    \end{align*}
    \begin{itemize}
        \item The last substitution above is legal because if $A=S\Lambda S^{-1}$, then $\Lambda=S^{-1}AS$.
    \end{itemize}
    \begin{align*}
        0 &=
        \begin{vmatrix}
            1-\lambda & 1\\
            4 & -2-\lambda\\
        \end{vmatrix}\\
        &= (1-\lambda)(-2-\lambda)-4\\
        &= -2-\lambda+2\lambda+\lambda^2-4\\
        &= \lambda^2+\lambda-6\\
        &= (\lambda-2)(\lambda+3)
    \end{align*}
    \begin{align*}
        \lambda_1 &= 2&
        \lambda_2 &= -3
    \end{align*}
    \begin{itemize}
        \item $
            A-2I=
            \begin{bmatrix}
                -1 & 1\\
                4 & -4\\
            \end{bmatrix}
            \begin{bmatrix}
                1\\
                1\\
            \end{bmatrix}
            =
            \begin{bmatrix}
                0\\
                0\\
            \end{bmatrix}
        $
        \item $
            A+3I=
            \begin{bmatrix}
                4 & 1\\
                4 & 1\\
            \end{bmatrix}
            \begin{bmatrix}
                -1\\
                4\\
            \end{bmatrix}
            =
            \begin{bmatrix}
                0\\
                0\\
            \end{bmatrix}
        $
    \end{itemize}
    \begin{align*}
        u' &= \Lambda u\\
        \begin{bmatrix}
            u_1'\\
            u_2'\\
        \end{bmatrix}
        &=
        \begin{bmatrix}
            2 & 0\\
            0 & -3\\
        \end{bmatrix}
        \begin{bmatrix}
            u_1\\
            u_2\\
        \end{bmatrix}
    \end{align*}
    \begin{align*}
        u_1 &= k_1\e^{2x}\\
        u_2 &= k_2\e^{-3x}
    \end{align*}
    \begin{align*}
        y &= Su\\
        \begin{bmatrix}
            y_1\\
            y_2\\
        \end{bmatrix}
        &=
        \begin{bmatrix}
            1 & -1\\
            1 & 4\\
        \end{bmatrix}
        \begin{bmatrix}
            k_1\e^{2x}\\
            k_2\e^{-3x}\\
        \end{bmatrix}
    \end{align*}
    \begin{align*}
        y_1 &= k_1\e^{2x}-k_2\e^{-3x}\\
        y_2 &= k_1\e^{2x}+4k_2\e^{-3x}
    \end{align*}

    %%%%%%%%%%%%%%%%%%%%%%

    \item \marginnote{2/12:}Initial conditions: $y_1(0)=1$ and $y_2(0)=6$.
    \begin{itemize}
        \item Use augmented matrices to solve a system of equations.
    \end{itemize}
    \begin{equation*}
        \begin{amatrix}{2}
            1 & -1 & 1\\
            1 & 4 & 6\\
        \end{amatrix}
        \rightarrow
        \begin{amatrix}{2}
            1 & 0 & 2\\
            0 & 1 & 1\\
        \end{amatrix}
    \end{equation*}
    \item Particular solution:
    \begin{align*}
        y_1 &= 2\e^{2x}-\e^{-3x}\\
        y_2 &= 2\e^{2x}+4\e^{-3x}
    \end{align*}
\end{itemize}


\subsection*{Matrix Exponentiation}
\begin{itemize}
    \item $\e^A$ is a matrix defined as the infinite sum of a power series.
    \item $f(t) = \e^t$.
\end{itemize}
\begin{tchart}{1.6}{Differential Equations}{Power Series}
    $f'(t)=f(t)$ & $f(t)=1+t+\frac{t^2}{2}+\frac{t^3}{3!}+\cdots$\\
    $f(0)=1$ & $\frac{\text{d}}{\text{d}t}(t)=1$, $\frac{\text{d}}{\text{d}t}\left( \frac{t^2}{2} \right)=t$, \dots\\
    & $f(t)=\sum_{n=0}^\infty \frac{t^n}{n!}$
\end{tchart}
\begin{itemize}
    \item $f(t)=\e^{at}$.
\end{itemize}
\begin{tchart}{1.5}{Differential Equations}{Power Series}
    $f'(t)=af(t)$ & $f(t)=\sum_{n=0}^\infty \frac{a^nt^n}{n!}$\\
    $f(0)=1$ &\\
\end{tchart}
\begin{itemize}
    \item $F(t)=\e^{At}$.
    \begin{itemize}
        \item A matrix-valued function.
        \item Ex. $
            F(\theta) =
            \begin{bmatrix}
                \cos(\theta) & -\sin(\theta)\\
                \sin(\theta) & \cos(\theta)\\
            \end{bmatrix}
        $
        \item $F(\theta)A$ rotates points (arrows) of $A$ by $\theta$.
    \end{itemize}
\end{itemize}
\begin{tchart}{1.5}{Differential Equations}{Power Series}
    $F'(t)=A\e^{At}$ & $F(t)=I+At+\frac{A^2t^2}{2!}+\frac{A^3t^3}{3!}+\cdots$\\
    $F(0)=I$ & $F(t)=\sum_{n=0}^\infty \frac{A^nt^n}{n!}$\\
\end{tchart}


\subsection*{Diagonalization of e\textsuperscript{\emph{At}}}
\begin{itemize}
    \item Find an alternate form for $\e^{At}$ by manipulating its power series definition:
    \begin{align*}
        \e^{At} &= \sum_{n=0}^\infty \frac{A^nt^n}{n!}\\
        &= \sum_{n=0}^\infty \frac{S\Lambda^nS^{-1}t^n}{n!}\\
        &= \sum_{n=0}^\infty S\left( \frac{\Lambda^nt^n}{n!} \right)S^{-1}\\
        &= S\left( {\color{grx}\sum_{n=0}^\infty \frac{t^n}{n!}\Lambda^n} \right)S^{-1}\\
        &= S\left( \sum_{n=0}^\infty \frac{t^n}{n!}
        \begin{bmatrix}
            \lambda_1 &  & \\
             & \ddots & \\
             &  & \lambda_k\\
        \end{bmatrix}^n
        \right)S^{-1}\\
        &= S\left( \sum_{n=0}^\infty \frac{t^n}{n!}
        \begin{bmatrix}
            \lambda_1^n &  & \\
             & \ddots & \\
             &  & \lambda_k^n\\
        \end{bmatrix}
        \right)S^{-1}\\
        &= S\left( \sum_{n=0}^\infty
        \begin{bmatrix}
            \frac{t^n}{n!}\lambda_1^n &  & \\
             & \ddots & \\
             &  & \frac{t^n}{n!}\lambda_k^n\\
        \end{bmatrix}
        \right)S^{-1}\\
        &= S\left( \sum_{n=0}^\infty
        \begin{bmatrix}
            \frac{\lambda_1^nt^n}{n!} &  & \\
             & \ddots & \\
             &  & \frac{\lambda_k^nt^n}{n!}\\
        \end{bmatrix}
        \right)S^{-1}\\
        %
        %%%%%%%%%%%%%%%%%%%%%%
        %
        \marginnote{2/13:}&= S
        \begin{bmatrix}
            \sum_{n=0}^\infty \frac{\lambda_1^nt^n}{n!} &  & \\
             & \ddots & \\
             &  & \sum_{n=0}^\infty \frac{\lambda_k^nt^n}{n!}\\
        \end{bmatrix}
        S^{-1}\\
        &= S
        \begin{bmatrix}
            \e^{\lambda_1t} &  & \\
             & \ddots & \\
             &  & \e^{\lambda_kt}\\
        \end{bmatrix}
        S^{-1}\\
        &= S{\color{grx}\e^{\Lambda t}}S^{-1}\\
        &= F(t)
    \end{align*}
    \item Prove, using the above result, that $F'(t)$ can be defined in terms of $F(t)$:
    \begin{align*}
        F(t) &= \e^{At}\\
        &= S
        \begin{bmatrix}
            \e^{\lambda_1t} &  & \\
             & \ddots & \\
             &  & \e^{\lambda_kt}\\
        \end{bmatrix}
        S^{-1}
    \end{align*}
    \begin{align*}
        F'(t) &= \dd{t}\left( S
        \begin{bmatrix}
            \e^{\lambda_1t} &  & \\
             & \ddots & \\
             &  & \e^{\lambda_kt}\\
        \end{bmatrix}
        S^{-1} \right)\\
        &= S\dd{t}\left(
        \begin{bmatrix}
            \e^{\lambda_1t} &  & \\
             & \ddots & \\
             &  & \e^{\lambda_kt}\\
        \end{bmatrix}
        \right)S^{-1}\\
        &= S
        \begin{bmatrix}
            \dd{t}\e^{\lambda_1t} &  & \\
             & \ddots & \\
             &  & \dd{t}\e^{\lambda_kt}\\
        \end{bmatrix}
        S^{-1}\\
        &= S
        \begin{bmatrix}
            \lambda_1\e^{\lambda_1t} &  & \\
             & \ddots & \\
             &  & \lambda_k\e^{\lambda_kt}\\
        \end{bmatrix}
        S^{-1}\\
        &= S
        \begin{bmatrix}
            \lambda_1 &  & \\
             & \ddots & \\
             &  & \lambda_k\\
        \end{bmatrix}
        \begin{bmatrix}
            \e^{\lambda_1t} &  & \\
             & \ddots & \\
             &  & \e^{\lambda_kt}\\
        \end{bmatrix}
        S^{-1}\\
        &= S
        \begin{bmatrix}
            \lambda_1 &  & \\
             & \ddots & \\
             &  & \lambda_k\\
        \end{bmatrix}
        I_k
        \begin{bmatrix}
            \e^{\lambda_1t} &  & \\
             & \ddots & \\
             &  & \e^{\lambda_kt}\\
        \end{bmatrix}
        S^{-1}\\
        &= {\color{red}S
        \begin{bmatrix}
            \lambda_1 &  & \\
             & \ddots & \\
             &  & \lambda_k\\
        \end{bmatrix}
        S^{-1}}{\color{blue}S
        \begin{bmatrix}
            \e^{\lambda_1t} &  & \\
             & \ddots & \\
             &  & \e^{\lambda_kt}\\
        \end{bmatrix}
        S^{-1}}\\
        &= {\color{red}A}{\color{blue}F(t)}\\
        &= A\e^{At}
    \end{align*}
    \item In other words, $y'(t)=Ay(t)$ and $y(0)=y_0$. The solution is $y=\e^{At}y_0$.
    \item Example:
    \begin{align*}
        y'_1 &= 5y_1+y_2& y_1(0) &= -3\\
        y'_2 &= -2y_1+2y_2& y_2(0) &= 8
    \end{align*}
    \begin{align*}
        y(t) &= \e^{At}y(0)\\
        &= S\e^{\Lambda t}S^{-1}y(0)
    \end{align*}
    \begin{align*}
        0 &= |A-\lambda I|\\
        &=
        \begin{vmatrix}
            5-\lambda & 1\\
            -2 & 2-\lambda\\
        \end{vmatrix}\\
        &= (\lambda-3)(\lambda-4)
    \end{align*}
    \begin{itemize}
        \item $
            A-3I =
            \begin{bmatrix}
                2 & 1\\
                -2 & -1\\
            \end{bmatrix}
        $
        \item $
            N(A-3I) = \left\{
            \begin{bmatrix}
                1\\
                -2\\
            \end{bmatrix}
            \right\}
        $
        \item $
            A-4I =
            \begin{bmatrix}
                1 & 1\\
                -2 & -2\\
            \end{bmatrix}
        $
        \item $
            N(A-4I) = \left\{
            \begin{bmatrix}
                -1\\
                1\\
            \end{bmatrix}
            \right\}
        $
    \end{itemize}
    \begin{align*}
        y(t) &= S\e^{\Lambda t}S^{-1}y(0)\\
        &=
        \begin{bmatrix}
            1 & -1\\
            -2 & 1\\
        \end{bmatrix}
        \begin{bmatrix}
            \e^{3t} & 0\\
            0 & \e^{4t}\\
        \end{bmatrix}
        \begin{bmatrix}
            -1 & -1\\
            -2 & -1\\
        \end{bmatrix}
        \begin{bmatrix}
            -3\\
            8\\
        \end{bmatrix}\\
        &=
        \begin{bmatrix}
            \e^{3t} & -\e^{4t}\\
            -2\e^{3t} & \e^{4t}\\
        \end{bmatrix}
        \begin{bmatrix}
            -1 & -1\\
            -2 & -1\\
        \end{bmatrix}
        \begin{bmatrix}
            -3\\
            8\\
        \end{bmatrix}\\
        &=
        \begin{bmatrix}
            -\e^{3t}+2\e^{4t} & -\e^{3t}+\e^{4t}\\
            2\e^{3t}-2\e^{4t} & 2\e^{3t}-\e^{4t}\\
        \end{bmatrix}
        \begin{bmatrix}
            -3\\
            8\\
        \end{bmatrix}\\
        &=
        \begin{bmatrix}
            3\e^{3t}-6\e^{4t}-8\e^{3t}+8\e^{4t}\\
            -6\e^{3t}+6\e^{4t}+16\e^{3t}-8\e^{4t}\\
        \end{bmatrix}\\
        &=
        \begin{bmatrix}
            -5\e^{3t}+2\e^{4t}\\
            10\e^{3t}-2\e^{4t}\\
        \end{bmatrix}\\
        &=
        \begin{bmatrix}
            y_1(t)\\
            y_2(t)\\
        \end{bmatrix}
    \end{align*}
\end{itemize}



\section*{Orthonormally Diagonalizable Matrices}
\begin{itemize}
    \item \marginnote{2/19:}$A=Q\Lambda Q^\T$.
    \begin{itemize}
        \item Eigenvectors are orthonormal.
    \end{itemize}
    \item $A^\T = \left( Q\Lambda Q^\T \right)^\T = Q^{\T\T}\Lambda^\T Q^\T = Q\Lambda Q^\T = A$.
    \item Prove that the symmetric matrices are exactly those that are orthonormally diagonalizable.
    \begin{itemize}
        \item Let $A = A^\T$.
        \begin{equation}\label{eqn:ortho1}
            Ax_1 = \lambda_1x_1
        \end{equation}
        \begin{equation}\label{eqn:ortho2}
            Ax_2 = \lambda_2x_2
        \end{equation}
        \item Multiply Equation \ref{eqn:ortho1} by $x_2^\T$ from Equation \ref{eqn:ortho2}.
        \begin{itemize}
            \item We have to relate the two equations.
            \item Later, we transpose, because we have to specifically target the properties of symmetric matrices.
        \end{itemize}
        \begin{align*}
            \lambda_1x_2^\T x_1 &= x_2^\T Ax_1\\
            &= (x_2^\T A)x_1\\
            &= \left( A^\T x_2 \right)^\T x_1\\
            &= \left( A x_2 \right)^\T x_1\\
            \lambda_1x_2^\T x_1 &= \lambda_2x_2^T x_1\\
            \lambda_1x_2^\T x_1-\lambda_2x_2^T x_1 &= 0\\
            x_2^\T x_1(\lambda_1-\lambda_2) &= 0
        \end{align*}
        \item The last line above implies that $x_2^\T x_1 = 0$ iff $\lambda_1\neq\lambda_2$.
    \end{itemize}
    \item The only matrices that we can guarantee will never have complex eigenvalues are symmetric matrices.
    \item On complex numbers/vectors:
    \begin{itemize}
        \item $z=a+bi$ and $\bar{z}=a-bi$, where $a,b\in\R$, $i=\sqrt{-1}$. Note that $\bar{z}$ is the \textbf{complex conjugate} of $z$.
        \item $z\bar{z} = a^2+b^2$.
        \item $
            x =
            \begin{bmatrix}
                a_1+b_1i\\
                \vdots\\
                a_n+b_ni\\
            \end{bmatrix}
        $ and $
            \bar{x} =
            \begin{bmatrix}
                a_1-b_1i\\
                \vdots\\
                a_n-b_ni\\
            \end{bmatrix}
        $.
    \end{itemize}
    \item Prove that when $A = A^\T$, $\lambda_n\in\R$.
    \begin{itemize}
        \item Let $A = A^\T$, $A\in\R^n$.
        \begin{equation}\label{eqn:ortho3}
            Ax = \lambda x
        \end{equation}
        \begin{equation*}
            \bar{A}\bar{x} = \bar{\lambda}\bar{x}
        \end{equation*}
        \item If $A\in\R^n$, then $A=\bar{A}$.
        \begin{align*}
            A\bar{x} &= \bar{\lambda}\bar{x}\\
            \left( A\bar{x} \right)^\T &= \left( \bar{\lambda}\bar{x} \right)^\T\\
            \bar{x}^\T A^\T &= \bar{\lambda}\bar{x}^\T\\
            \bar{x}^\T A &= \bar{\lambda}\bar{x}^\T\stepcounter{equation}\tag{\theequation}\label{eqn:ortho4}\\
        \end{align*}
        \item Multiply Equation \ref{eqn:ortho3} by $\bar{x}^\T$ from the left.
        \begin{itemize}
            \item ${\color{blue}\bar{x}^\T Ax} = {\color{red}\lambda\bar{x}^\T x}$.
        \end{itemize}
        \item Multiply Equation \ref{eqn:ortho4} by $x$ from the right.
        \begin{itemize}
            \item ${\color{blue}\bar{x}^\T Ax} = {\color{grx}\bar{\lambda}\bar{x}^\T x}$.
        \end{itemize}
        \item ${\color{red}\lambda\bar{x}^\T x} = {\color{grx}\bar{\lambda}\bar{x}^\T x} \Rightarrow \lambda = \bar{\lambda} \Rightarrow \lambda\in\R$.
    \end{itemize}
\end{itemize}



\subsection*{Spectral Decomposition}
\marginnote{2/20:}\begin{equation*}
    A =
    \begin{bmatrix}
        2 & 1 & 1\\
        1 & 2 & 1\\
        1 & 1 & 2\\
    \end{bmatrix}
\end{equation*}
\begin{itemize}
    \item $\lambda_1 = 4,\ \lambda_2=\lambda_3=1$.
    \item $
        x_1 =
        \begin{bmatrix}
            1\\
            1\\
            1\\
        \end{bmatrix}
    $, $
        x_2 =
        \begin{bmatrix}
            -1\\
            0\\
            1\\
        \end{bmatrix}
    $, $
        x_3 =
        \begin{bmatrix}
            -1\\
            1\\
            0\\
        \end{bmatrix}
    $.
    \begin{itemize}
        \item $x_1^\T x_2 = 0$, $x_1^\T x_3 = 0$, $x_2^\T x_3 = -1$.
    \end{itemize}
    \item Orthogonalize by Gram-Schmidt, inspection, put the vectors in a matrix and find the null space (the null vector will be orthogonal by the fundamental theorem).
    \item $
        x_3' =
        \begin{bmatrix}
            -1\\
            2\\
            -1\\
        \end{bmatrix}
    $.
    \begin{itemize}
        \item $x_3'$ is not scaled along its line by $A$, but it is scaled in the plane of $x_2$ and $x_3$ by $A$.
        \item $x_1^\T x_3' = 0$, $x_2^\T x_3' = 0$.
    \end{itemize}
    \item $
        q_1 = \frac{1}{\sqrt{3}}
        \begin{bmatrix}
            1\\
            1\\
            1\\
        \end{bmatrix}
    $, $
        q_2 = \frac{1}{\sqrt{2}}
        \begin{bmatrix}
            -1\\
            0\\
            1\\
        \end{bmatrix}
    $, $
        q_3 = \frac{1}{\sqrt{6}}
        \begin{bmatrix}
            -1\\
            2\\
            -1\\
        \end{bmatrix}
    $.
    \begin{align*}
        A &= Q\Lambda Q^\T\\
        &=
        \begin{bmatrix}
            | &  & |\\
            q_1 & \cdots & q_n\\
            | &  & |\\
        \end{bmatrix}
        \begin{bmatrix}
            \lambda_1 &  & \\
             & \ddots & \\
             &  & \lambda_n\\
        \end{bmatrix}
        \begin{bmatrix}
            \text{---} & q_1^\T & \text{---}\\
             & \vdots & \\
            \text{---} & q_n^\T & \text{---}\\
        \end{bmatrix}\\
        &=
        \begin{bmatrix}
            | &  & |\\
            q_1\lambda_1 & \cdots & q_n\lambda_n\\
            | &  & |\\
        \end{bmatrix}
        \begin{bmatrix}
            \text{---} & q_1^\T & \text{---}\\
             & \vdots & \\
            \text{---} & q_n^\T & \text{---}\\
        \end{bmatrix}\\
        &= \lambda_1q_1q_1^\T+\cdots+\lambda_nq_nq_n^\T
    \end{align*}
    \renewcommand{\arraystretch}{1.6}
    \item $
        q_1q_1^\T =
        \begin{bmatrix}
            \frac{1}{\sqrt{3}}\\
            \frac{1}{\sqrt{3}}\\
            \frac{1}{\sqrt{3}}\\
        \end{bmatrix}
        \begin{bmatrix}
            \frac{1}{\sqrt{3}} & \frac{1}{\sqrt{3}} & \frac{1}{\sqrt{3}}\\
        \end{bmatrix}
        =
        \begin{bmatrix}
            \frac{1}{3} & \frac{1}{3} & \frac{1}{3}\\
            \frac{1}{3} & \frac{1}{3} & \frac{1}{3}\\
            \frac{1}{3} & \frac{1}{3} & \frac{1}{3}\\
        \end{bmatrix}
    $
    \item $
        q_2q_2^\T =
        \begin{bmatrix}
            -\frac{1}{\sqrt{2}}\\
            0\\
            \frac{1}{\sqrt{2}}\\
        \end{bmatrix}
        \begin{bmatrix}
            -\frac{1}{\sqrt{2}} & 0 & \frac{1}{\sqrt{2}}\\
        \end{bmatrix}
        =
        \begin{bmatrix}
            \frac{1}{2} & 0 & -\frac{1}{2}\\
            0 & 0 & 0\\
            -\frac{1}{2} & 0 & \frac{1}{2}\\
        \end{bmatrix}
    $
    \item $
        q_3q_3^\T =
        \begin{bmatrix}
            -\frac{1}{\sqrt{6}}\\
            \frac{2}{\sqrt{6}}\\
            -\frac{1}{\sqrt{6}}\\
        \end{bmatrix}
        \begin{bmatrix}
            -\frac{1}{\sqrt{6}} & \frac{2}{\sqrt{6}} & -\frac{1}{\sqrt{6}}\\
        \end{bmatrix}
        =
        \begin{bmatrix}
            \frac{1}{6} & -\frac{1}{3} & \frac{1}{6}\\
            -\frac{1}{3} & \frac{2}{3} & -\frac{1}{3}\\
            \frac{1}{6} & -\frac{1}{3} & \frac{1}{6}\\
        \end{bmatrix}
    $
    \item $
        A = 4
        \begin{bmatrix}
            \frac{1}{3} & \frac{1}{3} & \frac{1}{3}\\
            \frac{1}{3} & \frac{1}{3} & \frac{1}{3}\\
            \frac{1}{3} & \frac{1}{3} & \frac{1}{3}\\
        \end{bmatrix}
        +
        \begin{bmatrix}
            \frac{1}{2} & 0 & -\frac{1}{2}\\
            0 & 0 & 0\\
            -\frac{1}{2} & 0 & \frac{1}{2}\\
        \end{bmatrix}
        +
        \begin{bmatrix}
            \frac{1}{6} & -\frac{1}{3} & \frac{1}{6}\\
            -\frac{1}{3} & \frac{2}{3} & -\frac{1}{3}\\
            \frac{1}{6} & -\frac{1}{3} & \frac{1}{6}\\
        \end{bmatrix}
    $
    \item $
        A = 4
        \begin{bmatrix}
            \frac{1}{3} & \frac{1}{3} & \frac{1}{3}\\
            \frac{1}{3} & \frac{1}{3} & \frac{1}{3}\\
            \frac{1}{3} & \frac{1}{3} & \frac{1}{3}\\
        \end{bmatrix}
        +
        \begin{bmatrix}
            \frac{2}{3} & -\frac{1}{3} & -\frac{1}{3}\\
            -\frac{1}{3} & \frac{2}{3} & -\frac{1}{3}\\
            -\frac{1}{3} & -\frac{1}{3} & \frac{2}{3}\\
        \end{bmatrix}
    $ is the spectral decomposition of $A$.
    \renewcommand{\arraystretch}{1}
\end{itemize}



\includepdf[pages=-]{PDFInserts/QuadricSurfacesSketching.pdf}



\section*{Quadric Forms / Positive Definite Matrices}
\begin{itemize}
    \item \marginnote{2/24:}Motivation: Find bases for conic sections that eliminate rotation terms.
    \renewcommand{\arraystretch}{1.6}
    \item Let $
        x =
        \begin{bmatrix}
            x\\
            y\\
            z\\
        \end{bmatrix}
    $, $
        A =
        \begin{bmatrix}
            a & \frac{d}{2} & \frac{e}{2}\\
            \frac{d}{2} & b & \frac{f}{2}\\
            \frac{e}{2} & \frac{f}{2} & c\\
        \end{bmatrix}
    $.
    \begin{align*}
        x^\T Ax &=
        \renewcommand{\arraystretch}{1}
        \begin{bmatrix}
            x & y & z\\
        \end{bmatrix}
        \renewcommand{\arraystretch}{1.6}
        \begin{bmatrix}
            a & \frac{d}{2} & \frac{e}{2}\\
            \frac{d}{2} & b & \frac{f}{2}\\
            \frac{e}{2} & \frac{f}{2} & c\\
        \end{bmatrix}
        \begin{bmatrix}
            x\\
            y\\
            z\\
        \end{bmatrix}\\
        &=
        \renewcommand{\arraystretch}{1}
        \begin{bmatrix}
            ax+\frac{d}{2}y+\frac{e}{2}z & \frac{d}{2}x+by+\frac{f}{2}z & \frac{e}{2}x+\frac{f}{2}y+zc
        \end{bmatrix}
        \begin{bmatrix}
            x\\
            y\\
            z\\
        \end{bmatrix}\\
        &= x\left( ax+\frac{d}{2}y+\frac{e}{2}z \right)+y\left( \frac{d}{2}x+by+\frac{f}{2}z \right)+z\left( \frac{e}{2}x+\frac{f}{2}y+zc \right)\\
        &= ax^2+\frac{d}{2}xy+\frac{e}{2}xz+\frac{d}{2}xy+by^2+\frac{f}{2}yz+\frac{e}{2}xz+\frac{f}{2}yz+cz^2\\
        &= ax^2+by^2+cz^2+dxy+exz+fyz
    \end{align*}
    \item Example:
    \begin{equation*}
        f(x,y) = z = 5x^2+4xy+2y^2
    \end{equation*}
    \begin{itemize}
        \item An elliptic paraboloid (but not necessary to know this).
        \begin{align*}
            x^\T Ax &=
            \renewcommand{\arraystretch}{1}
            \begin{bmatrix}
                x & y\\
            \end{bmatrix}
            \renewcommand{\arraystretch}{1.6}
            \begin{bmatrix}
                5 & \frac{4}{2}\\
                \frac{4}{2} & 2\\
            \end{bmatrix}
            \begin{bmatrix}
                x\\
                y\\
            \end{bmatrix}\\
            &=
            \renewcommand{\arraystretch}{1}
            \begin{bmatrix}
                x & y\\
            \end{bmatrix}
            \begin{bmatrix}
                5 & 2\\
                2 & 2\\
            \end{bmatrix}
            \begin{bmatrix}
                x\\
                y\\
            \end{bmatrix}
        \end{align*}
        \item Let $x=Qy\Rightarrow y=Q^\T x$.
        \item If $A=A^\T$, then $A=Q\Lambda Q^\T\Rightarrow{\color{red}\Lambda}={\color{blue}Q^\T AQ}$.
        \begin{align*}
            x^\T Ax &= (Qy)^\T A(Qy)\\
            &= y^\T {\color{blue}Q^\T AQ}y\\
            &= y^\T{\color{red}\Lambda} y
        \end{align*}
        \renewcommand{\arraystretch}{1}
        \item Diagonalize $A$.
        \begin{align*}
            0 &= |A-\lambda I|\\
            &=
            \begin{vmatrix}
                5-\lambda & 2\\
                2 & 2-\lambda\\
            \end{vmatrix}\\
            &= (5-\lambda)(2-\lambda)-4\\
            &= \lambda^2-7\lambda+6
        \end{align*}
        \begin{align*}
            \lambda_1 &= 1 & \lambda_2 &= 6
        \end{align*}
        \item $
            N(A-I) = N\left( 
                \begin{bmatrix}
                    4 & 2\\
                    2 & 1\\
                \end{bmatrix}
            \right)
            =\left\{ 
                \begin{bmatrix}
                    1\\
                    -2\\
                \end{bmatrix}
            \right\}
        $
        \item $
            N(A-6I) = N\left( 
                \begin{bmatrix}
                    -1 & 2\\
                    2 & -4\\
                \end{bmatrix}
            \right)
            =\left\{ 
                \begin{bmatrix}
                    2\\
                    1\\
                \end{bmatrix}
            \right\}
        $
        \renewcommand{\arraystretch}{1.6}
        \item $
            q_1 =
            \begin{bmatrix}
                \frac{1}{\sqrt{5}}\\
                -\frac{2}{\sqrt{5}}\\
            \end{bmatrix}
        $
        \item $
            q_2 =
            \begin{bmatrix}
                \frac{2}{\sqrt{5}}\\
                \frac{1}{\sqrt{5}}\\
            \end{bmatrix}
        $
        \renewcommand{\arraystretch}{1}
        \item On standard basis: $f(x,y)=5x^2+4xy+4y^2$.
        \item On basis $Q$: $f(x,y)=x^2+6y^2$.
        \item We're rotating the $x$ and $y$ axes by the same angle and keeping them orthogonal:
        \begin{center}
            \begin{tikzpicture}
                \draw [blue] (-2,0) -- (2,0)node[right]{
                    $
                        \begin{bmatrix}
                            1\\
                            0\\
                        \end{bmatrix}
                    $
                };
                \draw [blue] (0,-2) -- (0,2)node[above]{
                    $
                        \begin{bmatrix}
                            0\\
                            1\\
                        \end{bmatrix}
                    $
                };

                \draw [red] (-2,-1) -- (2,1)node[right]{
                    $
                        \begin{bmatrix}
                            2\\
                            1\\
                        \end{bmatrix}
                    $
                };
                \draw [red] (-1,2) -- (1,-2)node[right]{
                    $
                        \begin{bmatrix}
                            1\\
                            -2\\
                        \end{bmatrix}
                    $
                };
            \end{tikzpicture}
        \end{center}
    \end{itemize}
    \item How are they equivalent?
    \begin{itemize}
        \item Let $
            x=
            \begin{bmatrix}
                1\\
                3\\
            \end{bmatrix}
        $
        \item $x^\T Ax = 5(1)^2+4(1)(3)+2(3^2) = 35$.
        \item $
            y = Q^\T x =
            \begin{bmatrix}
                \frac{1}{\sqrt{5}} & -\frac{2}{\sqrt{5}}\\
                \frac{2}{\sqrt{5}} & \frac{1}{\sqrt{5}}\\
            \end{bmatrix}
            \begin{bmatrix}
                1\\
                3\\
            \end{bmatrix}
            =
            \begin{bmatrix}
                -\frac{5}{\sqrt{5}}\\
                \frac{5}{\sqrt{5}}\\
            \end{bmatrix}
            =
            \begin{bmatrix}
                -\sqrt{5}\\
                \sqrt{5}\\
            \end{bmatrix}
        $
        \item $y^\T \Lambda y = \left( -\sqrt{5} \right)^2+6\left( \sqrt{5} \right)^2 = 35$.
    \end{itemize}
\end{itemize}


\subsection*{Sketching a Rotated Conic}
\begin{itemize}
    \item \marginnote{2/25:}Principal Axes Theorem: $x^\T Ax \rightarrow y^\T\Lambda y$.
    \item Consider $13x^2-10xy+13y^2-72=0$.
    \begin{itemize}
        \item We want to reexpress this using the Principal Axes Theorem to deal with the rotation term.
        \item $
            x^\T Ax = 72 =
            \begin{bmatrix}
                x & y\\
            \end{bmatrix}
            \begin{bmatrix}
                13 & -5\\
                -5 & 13\\
            \end{bmatrix}
            \begin{bmatrix}
                x\\
                y\\
            \end{bmatrix}
        $
        \item $0 = |A-\lambda I| = (\lambda-8)(\lambda-18)$.
        \begin{align*}
            \lambda_1 &= 8 & \lambda_2 &= 18\\
            x_1 &=
            \begin{bmatrix}
                1\\
                1\\
            \end{bmatrix}&
            x_2 &=
            \begin{bmatrix}
                -1\\
                1\\
            \end{bmatrix}
        \end{align*}
        \item Let $
            y=
            \begin{bmatrix}
                x'\\
                y'\\
            \end{bmatrix}
        $, $
            y^\T\Lambda y =
            \begin{bmatrix}
                x' & y'\\
            \end{bmatrix}
            \begin{bmatrix}
                8 & 0\\
                0 & 18\\
            \end{bmatrix}
            \begin{bmatrix}
                x'\\
                y'\\
            \end{bmatrix}
            =72
        $.
        \item $8(x')^2+18(y')^2=72$.
        \item $\left( \frac{x'}{3} \right)^2+\left( \frac{y'}{2} \right)^2=1$.
    \end{itemize}
    \item Sidenote on rotation matrices:
    \begin{itemize}
        \item No rotation (rotates $
            \begin{bmatrix}
                0^\circ & 0^\circ\\
                0^\circ & 0^\circ\\
            \end{bmatrix}
        $): $
            \begin{bmatrix}
                1 & 0\\
                0 & 1\\
            \end{bmatrix}
            =
            \begin{bmatrix}
                \cos\theta & -\sin\theta\\
                \sin\theta & \cos\theta\\
            \end{bmatrix}
        $
        \begin{itemize}
            \item Identity matrix is special case of rotation matrix when $\theta=0^\circ$.
        \end{itemize}
        \item Any rotation matrix has $|R|=1$ because otherwise, it would do something to the size.
    \end{itemize}
    \item Back with the example:
    \begin{itemize}
        \renewcommand{\arraystretch}{1.6}
        \item $
            Q =
            \begin{bmatrix}
                \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}}\\
                \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\\
            \end{bmatrix}
        $
        \renewcommand{\arraystretch}{1}
        \item $|Q|=1$.
        \item When $Q$ is set equivalent to the general rotation matrix (above), it is shown that $\theta=\frac{\pi}{4}$.
        \begin{center}
            \begin{tikzpicture}
                \draw (-2,0) -- (2,0) node[right]{$x$};
                \draw (0,-2) -- (0,2) node[above]{$y$};

                \draw [dashed] (-2,-2) -- (2,2) node[right]{$x'$};
                \draw [dashed] (2,-2) -- (-2,2) node[left]{$y'$};

                \begin{scope}[
                    rotate=45
                ]
                    \foreach \x in {-1.5,-1,-0.5,0.5,1,1.5} {
                        \draw [blue,thick] (\x,-0.1) -- (\x,0.1);
                    }
                    \foreach \y in {-1,-0.5,0.5,1} {
                        \draw [blue,thick] (-0.1,\y) -- (0.1,\y);
                    }

                    \fill [blue] (-1.5,0) circle (2pt);
                    \fill [blue] (1.5,0) circle (2pt);
                    \fill [blue] (0,-1) circle (2pt);
                    \fill [blue] (0,1) circle (2pt);

                    \draw [blue,thick] ellipse (1.5cm and 1cm);
                \end{scope}
            \end{tikzpicture}
        \end{center}
        \item If the sign of $|Q|$ is wrong, flip the columns (flips the sign of the determinant).
        \renewcommand{\arraystretch}{1.8}
        \begin{center}
            \begin{tabular}{c|c|c|c}
                $\lambda$'s & $\lambda_1 = 8$, $\lambda_2 = 18$ & $\lambda_1 = 18$, $\lambda_2 = 8$ & $\lambda_1 = 18$, $\lambda_2 = 8$\\
                \hline
                $Q$ & $
                    \begin{bmatrix}
                        -\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\\
                        -\frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}}\\
                    \end{bmatrix}
                $ & $
                    \begin{bmatrix}
                        -\frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}}\\
                        \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}}\\
                    \end{bmatrix}
                $ & $
                    \begin{bmatrix}
                        \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\\
                        -\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\\
                    \end{bmatrix}
                $\\
                \hline
                $\theta$ & $\frac{5\pi}{4}$ & $\frac{3\pi}{4}$ & $\frac{7\pi}{4}$\\
                \hline
                Graph & \tikz[scale=0.5]{
                    \draw (-2,0) -- (2,0);
                    \draw (0,-2) -- (0,2);
                    \draw [white] (0,2) -- (0,2.3);

                    \draw [dashed] (-1.41,-1.41) -- (1.41,1.41) node[right]{$x'$};
                    \draw [dashed] (1.41,-1.41) -- (-1.41,1.41) node[left]{$y'$};

                    \draw [thick,->] (0,1.2) arc[start angle=90,end angle=315,radius=1.2cm];
                    \draw [thick,->] (0.8,0) arc[start angle=0,end angle=225,radius=0.8cm];
                } & \tikz[scale=0.5]{
                    \draw (-2,0) -- (2,0);
                    \draw (0,-2) -- (0,2);
                    \draw [white] (0,2) -- (0,2.3);

                    \draw [dashed] (-1.41,-1.41) -- (1.41,1.41) node[right]{$y'$};
                    \draw [dashed] (1.41,-1.41) -- (-1.41,1.41) node[left]{$x'$};

                    \draw [thick,->] (0,1.2) arc[start angle=90,end angle=225,radius=1.2cm];
                    \draw [thick,->] (0.8,0) arc[start angle=0,end angle=135,radius=0.8cm];
                } & \tikz[scale=0.5]{
                    \draw (-2,0) -- (2,0);
                    \draw (0,-2) -- (0,2);
                    \draw [white] (0,2) -- (0,2.3);

                    \draw [dashed] (-1.41,-1.41) -- (1.41,1.41) node[right]{$y'$};
                    \draw [dashed] (1.41,-1.41) -- (-1.41,1.41) node[left]{$x'$};

                    \draw [thick,->] (0,1.2) arc[start angle=90,end angle=405,radius=1.2cm];
                    \draw [thick,->] (0.8,0) arc[start angle=0,end angle=315,radius=0.8cm];
                }\\
                \hline
                Eq. & $\left( \frac{x'}{3} \right)^2+\left( \frac{y'}{2} \right)^2=1$ & $\left( \frac{x'}{2} \right)^2+\left( \frac{y'}{3} \right)^2=1$ & $\left( \frac{x'}{2} \right)^2+\left( \frac{y'}{3} \right)^2=1$\\
            \end{tabular}
        \end{center}
    \end{itemize}
    \item Classification of symmetric matrices:
    \renewcommand{\arraystretch}{1.4}
    \begin{center}
        \begin{tabular}{l|c|c}
            Classification & $x^\T Ax$ & $\lambda$'s\\
            \hline
            Positive definite & $f=x^\T Ax$, $f>0$ & All $\lambda\text{'s}>0$\\
            \hline
            Positive semidefinite & $f\geq 0$ & All $\lambda\text{'s}\geq0$\\
            \hline
            Negative definite & $f<0$ & All $\lambda\text{'s}<0$\\
            \hline
            Negative semidefinite & $f\leq 0$ & All $\lambda\text{'s}\leq0$\\
            \hline
            Indefinite & $f$ is positive and negative & $\lambda\text{'s}\in\R$\\
        \end{tabular}
    \end{center}
\end{itemize}



\section*{Nondiagonalizable Matrices}
\subsection*{Perturbations}
\begin{itemize}
    \item \marginnote{2/26:}If it's nondiagonalizable, how can we get it to as close to diagonalizable as possible? If that doesn't help, how can we get it into a useful form?
    \begin{equation*}
        A=
        \begin{bmatrix}
            1 & 1\\
            0 & 1\\
        \end{bmatrix}
    \end{equation*}
    \item $A$ is nondiagonalizable because the eigenvalues are sitting on the principal diagonal already, and they're the same. We want distinct eigenvalues.
    \item Let $\epsilon$ be a very small, approaching zero quantity.
    \begin{equation*}
        A_\epsilon =
        \begin{bmatrix}
            1 & 1\\
            \epsilon^2 & 1\\
        \end{bmatrix}
    \end{equation*}
    \begin{itemize}
        \item Choice of $\epsilon^2$ vs. $\epsilon$ helps when diagonalizing.
        \begin{align*}
            0 &= |A_\epsilon-\lambda I|\\
            &=
            \begin{vmatrix}
                1-\lambda & 1\\
                \epsilon^2 & 1-\lambda\\
            \end{vmatrix}\\
            &= (1-\lambda)^2-\epsilon^2\\
            &= 1-2\lambda+\lambda^2-\epsilon^2\\
            &= \lambda^2-2\lambda+(1-\epsilon)^2\\
            &= (\lambda-(1+\epsilon))(\lambda-(1-\epsilon))
        \end{align*}
        \item Quadratic formula may be helpful for factoring.
        \item Alternatively, think of it as a kind of difference of squares with a middle component.
    \end{itemize}
    \begin{align*}
        \lambda_1 &= 1+\epsilon & \lambda_2 &= 1-\epsilon
    \end{align*}
    \item $
        A_\epsilon-\lambda_1 I =
        \begin{bmatrix}
            -\epsilon & 1\\
            \epsilon^2 & -\epsilon\\
        \end{bmatrix}
    $, $
        x_1 =
        \begin{bmatrix}
            1\\
            \epsilon\\
        \end{bmatrix}
    $
    \item $
        A_\epsilon-\lambda_2 I =
        \begin{bmatrix}
            \epsilon & 1\\
            \epsilon^2 & \epsilon\\
        \end{bmatrix}
    $, $
        x_2 =
        \begin{bmatrix}
            1\\
            -\epsilon\\
        \end{bmatrix}
    $
    \item Find $S^{-1}$.
    \begin{itemize}
        \item $
            S^{-1} = \frac{1}{|S|}C^\T = -\frac{1}{2\epsilon}
            \begin{bmatrix}
                -\epsilon & -1\\
                -\epsilon & 1\\
            \end{bmatrix}
        $
    \end{itemize}
    \item $
        A_\epsilon = S\Lambda S^{-1} = -\frac{1}{2\epsilon}
        \begin{bmatrix}
            1 & 1\\
            \epsilon & -\epsilon\\
        \end{bmatrix}
        \begin{bmatrix}
            1+\epsilon & 0\\
            0 & 1-\epsilon\\
        \end{bmatrix}
        \begin{bmatrix}
            -\epsilon & -1\\
            -\epsilon & 1\\
        \end{bmatrix}
    $
    \item $
        A_\epsilon^n = -\frac{1}{2\epsilon}
        \begin{bmatrix}
            1 & 1\\
            \epsilon & -\epsilon\\
        \end{bmatrix}
        \begin{bmatrix}
            (1+\epsilon)^n & 0\\
            0 & (1-\epsilon)^n\\
        \end{bmatrix}
        \begin{bmatrix}
            -\epsilon & -1\\
            -\epsilon & 1\\
        \end{bmatrix}
    $
    \item Goal: In $A$, $\epsilon^2=0$. So we want $\lim_{\epsilon\to 0}A_\epsilon^n$.
    \begin{align*}
        \lim_{\epsilon\to 0} A_\epsilon^n &= \lim_{\epsilon\to 0} -\frac{1}{2\epsilon}
        \begin{bmatrix}
            1 & 1\\
            \epsilon & -\epsilon\\
        \end{bmatrix}
        \begin{bmatrix}
            (1+\epsilon)^n & 0\\
            0 & (1-\epsilon)^n\\
        \end{bmatrix}
        \begin{bmatrix}
            -\epsilon & -1\\
            -\epsilon & 1\\
        \end{bmatrix}\\
        &= \lim_{\epsilon\to 0} -\frac{1}{2\epsilon}
        \begin{bmatrix}
            1 & 1\\
            \epsilon & -\epsilon\\
        \end{bmatrix}
        \begin{bmatrix}
            -\epsilon(1+\epsilon)^n & -(1+\epsilon)^n\\
            -\epsilon(1-\epsilon)^n & (1-\epsilon)^n\\
        \end{bmatrix}\\
        &= \lim_{\epsilon\to 0} -\frac{1}{2\epsilon}
        \begin{bmatrix}
            -\epsilon(1+\epsilon)^n-\epsilon(1-\epsilon)^n & -(1+\epsilon)^n+(1-\epsilon)^n\\
            -\epsilon^2(1+\epsilon)^n+\epsilon^2(1-\epsilon)^n & -\epsilon(1+\epsilon)^n-\epsilon(1-\epsilon)^n\\
        \end{bmatrix}\\
        &= \lim_{\epsilon\to 0}
        \renewcommand{\arraystretch}{1.6}
        \begin{bmatrix}
            \frac{(1+\epsilon)^n+(1-\epsilon)^n}{2} & \frac{(1+\epsilon)^n-(1-\epsilon)^n}{2\epsilon}\\
            \frac{\epsilon(1+\epsilon)^n-\epsilon(1-\epsilon)^n}{2} & \frac{(1+\epsilon)^n+(1-\epsilon)^n}{2}\\
        \end{bmatrix}\\
        &=
        \begin{bmatrix}
            1 & \lim_{\epsilon\to 0} \frac{(1+\epsilon)^n-(1-\epsilon)^n}{2\epsilon}\\
            0 & 1\\
        \end{bmatrix}
    \end{align*}
    \begin{itemize}
        \item Use L'H\^{o}pital's rule for the last limit.
        \begin{align*}
            \dd{\epsilon}\left( (1+\epsilon)^n-(1-\epsilon)^n \right) &= \dd{\epsilon}(1+\epsilon)^n-\dd{\epsilon}(1-\epsilon)^n\\
            &= n(1+\epsilon)^{n-1}\dd{\epsilon}(1+\epsilon)-n(1-\epsilon)^{n-1}\dd{\epsilon}(1-\epsilon)\\
            &= n(1+\epsilon)^{n-1}(1)-n(1-\epsilon)^{n-1}(-1)\\
            &= n(1+\epsilon)^{n-1}+n(1-\epsilon)^{n-1}\\
            &= n\left( (1+\epsilon)^{n-1}+(1-\epsilon)^{n-1} \right)
        \end{align*}
        \begin{align*}
            \lim_{\epsilon\to 0} \frac{(1+\epsilon)^n-(1-\epsilon)^n}{2\epsilon} &= \lim_{\epsilon\to 0} \frac{n\left( (1+\epsilon)^{n-1}+(1-\epsilon)^{n-1} \right)}{2}\\
            &= \frac{2n}{2}\\
            &= n
        \end{align*}
        \item Thus, we find the following final formula for $A^n$.
        \begin{equation*}
            A^n =
            \begin{bmatrix}
                1 & n\\
                0 & 1\\
            \end{bmatrix}
        \end{equation*}
    \end{itemize}
\end{itemize}


\subsection*{Jordan Canonical Form}
\begin{itemize}
    \item \marginnote{2/27:}If $A$ is diagonalizable, $A=S\Lambda S^{-1}$.
    \begin{itemize}
        \item $
            \Lambda=
            \begin{bmatrix}
                \lambda_1 & & & 0\\
                 & \lambda_2 & & \\
                 & & \ddots & \\
                0 & & & \lambda_n\\
            \end{bmatrix}
        $
    \end{itemize}
    \item If $A$ is nondiagonalizable, $A=MJM^{-1}$\footnote{$J$ for Jordan matrix.}.
    \begin{itemize}
        \item $
            J=
            \begin{bmatrix}
                \lambda_1 & 1 & & 0\\
                 & \lambda_2 & &\\
                 &  & \ddots & 1\\
                0 &  &  & \lambda_n\\
            \end{bmatrix}
        $
        \item Has some 1s directly above the principal diagonal, notably above missing eigenvectors.
        \item Example: $
            J =
            \begin{bmatrix}
                \lambda_1 & 1\\
                0 & \lambda_2\\
            \end{bmatrix}
        $
        \item Example: $
            J =
            \begin{bmatrix}
                \lambda_1 & 0 & 0\\
                0 & \lambda_2 & 1\\
                0 & 0 & \lambda_3\\
            \end{bmatrix}
        $
        \item Note that if a Jordan form factorization is necessary, at least two $\lambda_1$, $\lambda_2$, and $\lambda_3$ are the same value. 
    \end{itemize}
\end{itemize}
\begin{tchart}{1.4}{Eigenvectors}{Generalized Eigenvectors / Power Vectors}
    $Ax=\lambda x$ & $(A-\lambda I)^px_m=0$\\
    $(A-\lambda I)x_s=0$ &\\
    $x_s$ is an eigenvector. &\\
\end{tchart}
\vspace{1em}
\begin{equation*}
    A=
    \begin{bmatrix}
        2 & 1\\
        0 & 2\\
    \end{bmatrix}
\end{equation*}
\begin{itemize}
    \item $\lambda_1=\lambda_2=2$.
    \item $
        A-2I =
        \begin{bmatrix}
            0 & 1\\
            0 & 0\\
        \end{bmatrix}
    $, so $
        x_1 =
        \begin{bmatrix}
            1\\
            0\\
        \end{bmatrix}
    $. This is an eigenvector.
    \item Now, note that $A-\lambda I$ raised to a power $p$ will eventually have a null space different from that of $A-\lambda I$ for some $p>1$ and $p\in\mathbb{N}$.
    \begin{itemize}
        \item Also note that $p\leq n$ for an $n$-square matrix.
        \item This implies that there exists a power vector distinct from the eigenvector(s) for some power $p$.
    \end{itemize}
    \item Let's find that power vector.
    \item $
        (A-2I)^2 =
        \begin{bmatrix}
            0 & 0\\
            0 & 0\\
        \end{bmatrix}
    $, so we can define a power vector $
        x_2 =
        \begin{bmatrix}
            0\\
            1\\
        \end{bmatrix}
    $. This P.V. has degree 2.
    \item Note that $A$ is already a Jordan matrix, so it makes sense that the eigenvectors/power vectors form the identity matrix. In other words, $M=M^{-1}=I_2$.
    \item Let's try another, less immediately clear example.
    \begin{equation*}
        A=
        \begin{bmatrix}
            3 & 1\\
            -1 & 1\\
        \end{bmatrix}
    \end{equation*}
    \item Since $A$ is neither upper triangular nor diagonal, we must derive the characteristic polynomial to find the eigenvectors.
    \begin{align*}
        0 &= |A-\lambda I|\\
        &= (3-\lambda)(1-\lambda)+1\\
        &= \lambda^2-4\lambda+4\\
        &= (\lambda-2)^2
    \end{align*}
    \begin{equation*}
        \lambda_1=\lambda_2=2
    \end{equation*}
    \item Since the algebraic multiplicity is 2 and the geometric multiplicity is only 1, find the one possible eigenvector.
    \begin{itemize}
        \item $
            A-2I=
            \begin{bmatrix}
                1 & 1\\
                -1 & -1\\
            \end{bmatrix}
        $, so $
            x_1 =
            \begin{bmatrix}
                1\\
                -1\\
            \end{bmatrix}
        $. This is an E.V.
    \end{itemize}
    \item We can also find a power vector.
    \begin{itemize}
        \item $
            (A-2I)^2 =
            \begin{bmatrix}
                0 & 0\\
                0 & 0\\
            \end{bmatrix}
        $, so $
            x_2=
            \begin{bmatrix}
                1\\
                0\\
            \end{bmatrix}
        $. This is a P.V. of degree 2.
    \end{itemize}
    \item We know that $Ax_1=2x_1$. However, in order to finish finding $J$, we need to (the reason will soon become clear) find $Ax_2$ in terms of $x_2$ and/or $x_1$. This can be done as follows.
    \begin{align*}
        (A-2I)^2x_2 &= 0\\
        (A-2I)(A-2I)x_2 &= (A-2I)x_1\\
        (A-2I)x_2 &= x_1\\
        Ax_2-2Ix_2 &= x_1\\
        Ax_2-2x_2 &= x_1\\
        Ax_2 &= x_1+2x_2
    \end{align*}
    \begin{itemize}
        \item Alternatively, we can think of starting from $(A-2I)x_2=x_1$ because we need to introduce the coefficient of 1 for one of the eigenvectors somehow.
        \item What combination of $A-\lambda I$ gives us another eigenvector?
    \end{itemize}
    \item Combine this result with $Ax_1=2x_1$ to yield the following set of equations.
    \begin{align*}
        Ax_1 &= 2x_1+0x_2\\
        Ax_2 &= x_1+2x_2
    \end{align*}
    \item Express the set of equations above explicitly in terms of the matrices that the variables represent.
    \begin{align*}
        \begin{bmatrix}
            3 & 1\\
            -1 & 1\\
        \end{bmatrix}
        \begin{bmatrix}
            1\\
            -1\\
        \end{bmatrix}
        &= 2
        \begin{bmatrix}
            1\\
            -1\\
        \end{bmatrix}
        +0
        \begin{bmatrix}
            1\\
            0\\
        \end{bmatrix}\\
        \begin{bmatrix}
            3 & 1\\
            -1 & 1\\
        \end{bmatrix}
        \begin{bmatrix}
            1\\
            0\\
        \end{bmatrix}
        &= 1
        \begin{bmatrix}
            1\\
            -1\\
        \end{bmatrix}
        +2
        \begin{bmatrix}
            1\\
            0\\
        \end{bmatrix}
    \end{align*}
    \item Rewrite the right side of the above set of equations to condense linear operations into matrix multiplication.
    \begin{align*}
        \begin{bmatrix}
            3 & 1\\
            -1 & 1\\
        \end{bmatrix}
        \begin{bmatrix}
            1\\
            -1\\
        \end{bmatrix}
        &=
        \begin{bmatrix}
            1 & 1\\
            -1 & 0\\
        \end{bmatrix}
        \begin{bmatrix}
            2\\
            0\\
        \end{bmatrix}\\
        \begin{bmatrix}
            3 & 1\\
            -1 & 1\\
        \end{bmatrix}
        \begin{bmatrix}
            1\\
            0\\
        \end{bmatrix}
        &=
        \begin{bmatrix}
            1 & 1\\
            -1 & 0\\
        \end{bmatrix}
        \begin{bmatrix}
            1\\
            2\\
        \end{bmatrix}
    \end{align*}
    \item Combine the two equations into one, giving $AM=MJ$.
    \begin{equation*}
        \begin{bmatrix}
            3 & 1\\
            -1 & 1\\
        \end{bmatrix}
        \begin{bmatrix}
            1 & 1\\
            -1 & 0\\
        \end{bmatrix}
        =
        \begin{bmatrix}
            1 & 1\\
            -1 & 0\\
        \end{bmatrix}
        \begin{bmatrix}
            2 & 1\\
            0 & 2\\
        \end{bmatrix}
    \end{equation*}
    \item Now, we have the Jordan matrix (the rightmost matrix above).
    \item Note that the following algebra would give us the $MJM^{-1}$ factorization of $A$.
    \begin{align*}
        AM &= MJ\\
        AMM^{-1} &= MJM^{-1}\\
        A &= MJM^{-1}
    \end{align*}
    \item Also note that if we find a coefficient matrix from the original set of equations and transpose it, we will get the Jordan matrix:
    \begin{equation*}
        \begin{bmatrix}
            2 & 0\\
            1 & 2\\
        \end{bmatrix}^\T
        = J =
        \begin{bmatrix}
            2 & 1\\
            0 & 2\\
        \end{bmatrix}
    \end{equation*}
    \item Let's try a bigger example.
    \begin{equation*}
        A=
        \begin{bmatrix}
            3 & 0 & 0\\
            0 & 4 & -1\\
            0 & 1 & 2\\
        \end{bmatrix}
    \end{equation*}
    \item $\lambda_1=\lambda_2=\lambda_3=3$
    \item $\text{A.M.}=3$, $\text{G.M.}=2$.
    \item $
        A-3I=
        \begin{bmatrix}
            0 & 0 & 0\\
            0 & 1 & -1\\
            0 & 1 & -1\\
        \end{bmatrix}
    $
    \item $
        x_1=
        \begin{bmatrix}
            1\\
            0\\
            0\\
        \end{bmatrix}
    $, $
        x_2=
        \begin{bmatrix}
            0\\
            1\\
            1\\
        \end{bmatrix}
    $
    \item Because $(A-3I)^2=0_3$ and because of the equivalence outlined in the previous example, we can use one of the following to find an $x_3$ that will suit out needs.
    \begin{itemize}
        \item $
            \begin{bmatrix}
                0 & 0 & 0\\
                0 & 1 & -1\\
                0 & 1 & -1\\
            \end{bmatrix}
            \begin{bmatrix}
                |\\
                x_3\\
                |\\
            \end{bmatrix}
            =
            \begin{bmatrix}
                1\\
                0\\
                0\\
            \end{bmatrix}
        $ or $
            \begin{bmatrix}
                0 & 0 & 0\\
                0 & 1 & -1\\
                0 & 1 & -1\\
            \end{bmatrix}
            \begin{bmatrix}
                |\\
                x_3\\
                |\\
            \end{bmatrix}
            =
            \begin{bmatrix}
                0\\
                1\\
                1\\
            \end{bmatrix}
        $
    \end{itemize}
    \item The first option will not work because no $x_3$ vector can get a 1 in the top position of $x_1$.
    \item For the second one, use inspection: $
        \begin{bmatrix}
            0 & 0 & 0\\
            0 & 1 & -1\\
            0 & 1 & -1\\
        \end{bmatrix}
        \begin{bmatrix}
            0\\
            1\\
            0\\
        \end{bmatrix}
        =
        \begin{bmatrix}
            0\\
            1\\
            1\\
        \end{bmatrix}
    $
    \begin{itemize}
        \item $x_3$ must be a power vector of $A-3I$ and must be independent of $x_1$ and $x_2$, so that $M$ is invertible.
    \end{itemize}
    \item Find $Ax_3$ in terms of $x_1$, $x_2$, and $x_3$.
    \begin{itemize}
        \item $(A-3I)x_3=x_2 \Rightarrow Ax_3-3x_3=x_2 \Rightarrow Ax_3=x_2+3x_3$.
    \end{itemize}
    \item Write the three equations as a set.
    \begin{align*}
        Ax_1 &= 3x_1+0x_2+0x_3\\
        Ax_2 &= 0x_1+3x_2+0x_3\\
        Ax_3 &= 0x_1+1x_2+3x_3
    \end{align*}
    \item Instead of going through converting this set of equations to matrix equations, simply take a coefficient matrix and transpose it to find $J$:
    \begin{itemize}
        \item $
            J=
            \begin{bmatrix}
                3 & 0 & 0\\
                0 & 3 & 1\\
                0 & 0 & 3\\
            \end{bmatrix}
        $
    \end{itemize}
    \item $
        A=MJM^{-1}=
        \begin{bmatrix}
            1 & 0 & 0\\
            0 & 1 & 1\\
            0 & 1 & 0\\
        \end{bmatrix}
        \begin{bmatrix}
            3 & 0 & 0\\
            0 & 3 & 1\\
            0 & 0 & 3\\
        \end{bmatrix}
        \begin{bmatrix}
            1 & 0 & 0\\
            0 & 0 & 1\\
            0 & 1 & -1\\
        \end{bmatrix}
    $
    \item Raising $J^n$ is easier than raising $A^n$, but it's still not easy.
    \item Note that $A^n=MJ^nM^{-1}$ and $
        J^n =
        \begin{bmatrix}
            3^n & 0 & 0\\
            0 & 3^n & n3^{n-1}\\
            0 & 0 & 3^n\\
        \end{bmatrix}
    $, as can be proved via induction.
    \begin{itemize}
        \item In other words, this factorization is useful.
        \item The $MJ^nM^{-1}$ factorization could also be condensed into a single matrix.
    \end{itemize}
\end{itemize}



\section*{Computational Complexity of the Jordan Canonical Form}
\begin{itemize}
    \item \marginnote{3/2:}Recall:
    \begin{itemize}
        \item Eigenvector: $Ax-\lambda x \Rightarrow A^nx_i=\lambda^n_ix_i \Rightarrow (A-\lambda I)x=0$.
        \item Power vector: $(A-\lambda I)^nx=0$.
    \end{itemize}
    \item Binomial expansion:
    \begin{itemize}
        \item Let $\binom{n}{r}={}_nC_r=\frac{n!}{r!(n-r)!}$.
        \item Theorem:
    \end{itemize}
    \begin{equation*}
        (x+y)^n = \binom{n}{0}x^n+\binom{n}{1}x^{n-1}y+\binom{n}{2}x^{n-2}y^2+\cdots+\binom{n}{n-1}xy^{n-1}+\binom{n}{n}y^n
    \end{equation*}
    \item Apply binomial expansion to finding $A^n$.
    \begin{align*}
        A^n &= (A-\lambda I+\lambda I)^n\\
        &= ((A-\lambda I)+(\lambda I))^n\\
        &= \binom{n}{0}(A-\lambda I)^n+\binom{n}{1}(A-\lambda I)^{n-1}(\lambda I)+\binom{n}{2}(A-\lambda I)^{n-2}(\lambda I)^2+\cdots+\binom{n}{n}(\lambda I)^n
    \end{align*}
    \item Now consider each term in the above expansion of $A^n$.
    \item Consider a degree 2 power vector.
    \begin{itemize}
        \item Let $x$ be a power vector of degree 2.
        \begin{itemize}
            \item Note that we are doing the binomial expansion in the reverse order in the following equation.
        \end{itemize}
        \begin{equation*}
            (A-\lambda I)^nx = {\color{red}\lambda^nIx+n\lambda^{n-1}(A-\lambda I)x}+{\color{blue}\binom{n}{2}\lambda^{n-1}(A-\lambda I)^2x}+{\color{grx}\cdots}+{\color{grx}\binom{n}{n}(A-\lambda I)^nx}
        \end{equation*}
        \item Because $x$ is a second-degree power vector and the blue term includes $(A-\lambda I)^2x$, the blue term equals zero.
        \item Furthermore, every subsequent term (the green terms) also equals zero. This is because the $(A-\lambda I)^kx$, $k\in[3,n]\cap\mathbb{N}$, term can always be factored into $(A-\lambda I)^{k-2}(A-\lambda I)^2x$, and, as we know, $(A-\lambda I)^2x=0$. If one part of the term is equal to zero, the whole term must be equal to zero as well.
        \item The implication is that only the red term remains. More succinctly:
        \begin{equation*}
            (A-\lambda I)^nx = {\color{red}\lambda^nIx+n\lambda^{n-1}(A-\lambda I)x}
        \end{equation*}
    \end{itemize}
    \item Let's generalize this to a degree $p$ power vector.
    \begin{itemize}
        \item Let $x$ be a power vector of degree $p$.
    \end{itemize}
    \begin{equation*}
        (A-\lambda I)^nx = \lambda^nIx+n\lambda^{n-1}(A-\lambda I)x+\cdots+\binom{n}{p-1}\lambda^{n-(p-1)}(A-\lambda I)^{p-1}x
    \end{equation*}
    \item Basically, the higher the degree of the power vector, the harder it is to compute the JCF.
    \item Note: $\e^{(A-\lambda I+\lambda I)t} = \e^{\lambda It}\e^{(A-\lambda I)t} = \e^{\lambda t}\left( \sum_{n=0}^\infty\frac{(A-\lambda I)^nt^n}{n!} \right)$.
\end{itemize}



\section*{Singular Value Decomposition}
\begin{itemize}
    \item \marginnote{3/5:}Let's deal with rectangular matrices.
    \item What we know:
    \begin{itemize}
        \item Regardless of the shape of $A_{m\times n}$, $A^\T A$ will be $n$-square and symmetric.
        \begin{itemize}
            \item From several chapters ago: $(A^\T A)^\T = A^\T A^{\T\T} = A^\T A$.
        \end{itemize}
        \item A symmetric matrix has a full set of real eigenvectors (guaranteed by spectral theorem).
        \item If $A=A^\T$, then $A=Q\Lambda Q^\T$ ($A$ is orthonormally diagonalizable).
    \end{itemize}
    \item We will show:
    \begin{itemize}
        \item Any $A_{m\times n}=QD^\dagger Q'^\T$.
        \begin{itemize}
            \item $D^\dagger$ is pseudo-diagonal (close to diagonal).
            \item $Q$ and $Q'^\T$$^[$\footnote{Que prime transpose.}$^]$ are slightly different sets of eigenvectors.
        \end{itemize}
    \end{itemize}
    \item Singular values:
    \begin{itemize}
        \item $A^\T A$ is symmetric.
        \item All $\lambda$'s are real.
        \item All eigenvectors ($(A-\lambda I)x=0$) can be chosen orthonormally.
        \item All $\lambda$'s are positive (shown by the following).
    \end{itemize}
    \item Let $x$ be a unit\footnote{Note that we are guaranteed that $x$ can have $||x||=1$ because we are guaranteed orthonormal eigenvectors.} eigenvector of $A^\T A$ with corresponding eigenvalue $\lambda$.
    \begin{itemize}
        \item Symbolically, $A^\T Ax=\lambda x$.
    \end{itemize}
    \begin{align*}
        0 &\leq ||Ax||^2\\
        &= (Ax)^\T(Ax)\\
        &= x^\T A^\T Ax\\
        &= x^\T\lambda x\\
        &= \lambda x^\T x\\
        &= \lambda(1)\\
        &= \lambda
    \end{align*}
    \item Singular values: $\sigma_i=\sqrt{\lambda_i}$.
    \item By convention, we list the singular values in descending order: $\sigma_1\geq\sigma_2\geq\dots\geq\sigma_n$.
    \item Let's try an example.
    \begin{equation*}
        A=
        \begin{bmatrix}
            1 & 1 & 0\\
            0 & 0 & 1\\
        \end{bmatrix}
    \end{equation*}
    \begin{itemize}
        \item Find $A^\T A$.
        \begin{equation*}
            A^\T A =
            \begin{bmatrix}
                1 & 0\\
                1 & 0\\
                0 & 1\\
            \end{bmatrix}
            \begin{bmatrix}
                1 & 1 & 0\\
                0 & 0 & 1\\
            \end{bmatrix}
            =
            \begin{bmatrix}
                1 & 1 & 0\\
                1 & 1 & 0\\
                0 & 0 & 1\\
            \end{bmatrix}
        \end{equation*}
        \item Find the eigenvectors of $A^\T A$.
        \begin{align*}
            \lambda_1 &= 2&
            \lambda_2 &= 1&
            \lambda_3 &= 0
        \end{align*}
        \item Find the singular values of $A^\T A$.
        \begin{align*}
            \sigma_1 &= \sqrt{2}&
            \sigma_2 &= 1&
            \sigma_3 &= 0
        \end{align*}
    \end{itemize}

    %%%%%%%%%%%%%%%%%%%

    \item \marginnote{3/6:}Let's step back for a second.
    \item The SVD is canonically written $A=U\Sigma V^\T$.
    \begin{itemize}
        \item If $A$ is $m\times n$:
        \begin{itemize}
            \item $U$ is $m\times m$ and orthonormal.
            \item $\Sigma$ is $m\times n$.
            \item $V$ is $n\times n$ and orthonormal.
        \end{itemize}
    \end{itemize}
    \item $\Sigma$ has the following form.
    \begin{center}
        \begin{tikzpicture}[
            node distance=4mm
        ]
            \matrix (Sigma) [matrix of math nodes,left delimiter={[},right delimiter={]},nodes in empty cells]
            {
                \sigma_1 & & 0 &\\
                 & \sigma_2 & & 0\\
                0 & & \ddots &\\
                 & 0 & & 0\\
            };
            \draw ([yshift=2mm]$ (Sigma-1-3)!0.5!(Sigma-1-4) $) -- ([yshift=-2mm]$ (Sigma-4-3)!0.5!(Sigma-4-4) $);
            \draw ([xshift=-2mm]$ (Sigma-3-1)!0.5!(Sigma-4-1) $) -- ([xshift=2mm]$ (Sigma-3-4)!0.5!(Sigma-4-4) $);

            \node [left=of Sigma,xshift=2mm] {$\Sigma =$};
            \node [above=of Sigma-1-2,anchor=south] {$r$};
            \node [above=of Sigma-1-4,anchor=south] {$n-r$};
            \node [right=of Sigma-2-4,anchor=west] {$r$};
            \node [right=of Sigma-4-4,anchor=west] {$m-r$};
        \end{tikzpicture}
    \end{center}
    \begin{itemize}
        \item All of the nonzero singular values go diagonally in the upper-left block.
        \item Zeroes everywhere else.
    \end{itemize}
    \item Continuing with the previous example:
    \begin{itemize}
        \item For $V$, form an orthonormal basis of eigenvectors of $A^\T A$.
        \item $
            x_1 =
            \begin{bmatrix}
                1\\
                1\\
                0\\
            \end{bmatrix}
        $, $
            x_2 =
            \begin{bmatrix}
                0\\
                0\\
                1\\
            \end{bmatrix}
        $, $
            x_3 =
            \begin{bmatrix}
                -1\\
                1\\
                0\\
            \end{bmatrix}
        $
        \renewcommand{\arraystretch}{1.6}
        \item $
            V =
            \begin{bmatrix}
                \frac{1}{\sqrt{2}} & 0 & \frac{-1}{\sqrt{2}}\\
                \frac{1}{\sqrt{2}} & 0 & \frac{1}{\sqrt{2}}\\
                0 & 1 & 0\\
            \end{bmatrix}
        $
        \renewcommand{\arraystretch}{1}
        \item Note that we put the eigenvectors in $V$ (not $U$) because the eigenvectors are $n$-dimensional and $V$ is the only $n$-square matrix (recall $U$ is $m$-square).
        \item For $U$, manipulate the eigenvectors by using them to rearrange $C(A)$.
        \item Note that $\left\{ Av_1,Av_2,\dots,Av_n \right\}$ is still an orthogonal basis for $\R^m$.
        \begin{itemize}
            \item This is because $A$ scales each orthogonal vector by the same amount, but brings them into $C(A)\in\R^m$.
            \item Furthermore, the following proves that orthogonality is maintained.
        \end{itemize}
        \begin{align*}
            (Av_i)^\T(Av_j) &= v_i^\T A^\T Av_j\\
            &= v_i^\T\lambda v_j\\
            &= \lambda v_i^\T v_j\\
            &= \lambda(0)\\
            &= 0
        \end{align*}
        \item Since $\sigma_i=||Av_i||$, set $u_i=\frac{1}{\sigma_i}Av_i$ to normalize $Av_i$ into the correct dimension.
        \item Essentially, the singular values allow us to define $U$ as follows.
        \begin{align*}
            Av_i &= \sigma_iu_i\\
            AV &= U\Sigma
        \end{align*}
        \begin{align*}
            A
            \begin{bmatrix}
                | &  & |\\
                v_1 & \cdots & v_n\\
                | &  & |\\
            \end{bmatrix}
            &=
            \begin{bmatrix}
                | &  & |\\
                Av_1 & \cdots & Av_n\\
                | &  & |\\
            \end{bmatrix}\\
            &=
            \begin{bmatrix}
                | &  & |\\
                \sigma_1u_1 & \cdots & \sigma_nu_n\\
                | &  & |\\
            \end{bmatrix}\\
            &=
            \begin{bmatrix}
                | &  & |\\
                u_1 & \cdots & u_n\\
                | &  & |\\
            \end{bmatrix}
            \begin{bmatrix}
                \sigma_1 & & &\\
                 & \sigma_2 & & 0\\
                 & & \ddots &\\
                 & 0 & & 0\\
            \end{bmatrix}
        \end{align*}
    \end{itemize}
    \item Note: If $A=U\Sigma V^\T$, then the following holds.
    \begin{align*}
        A^\T A &= (U\Sigma V^\T)^\T(U\Sigma V^\T)\\
        &= V\sigma^\T U^\T U\Sigma V^\T\\
        &= V\Sigma\Sigma V^\T\\
        &= V\Sigma^2V^\T\\
        &= V\Lambda V^\T
    \end{align*}
    \item In the opposite direction:
    \begin{align*}
        AA^\T &= (U\Sigma V^\T)(U\Sigma V^\T)^\T\\
        &= \vdots\\
        &= U\Lambda U^\T
    \end{align*}
    \begin{itemize}
        \item Back to the example to finish it.
        \item The following finds $u_1$. This process can be iterated for $u_i$.
        \item $
            u_1 = \frac{1}{\sigma_1}Av_1 = \frac{1}{\sqrt{2}}
            \begin{bmatrix}
                1 & 1 & 0\\
                0 & 0 & 1\\
            \end{bmatrix}
            \renewcommand{\arraystretch}{1.6}
            \begin{bmatrix}
                \frac{1}{\sqrt{2}}\\
                \frac{1}{\sqrt{2}}\\
                0\\
            \end{bmatrix}
            \renewcommand{\arraystretch}{1}
            =
            \begin{bmatrix}
                0\\
                1\\
            \end{bmatrix}
        $
        \item Assemble the final factorization.
        \item $
            A = U\Sigma V^\T =
            \begin{bmatrix}
                1 & 0\\
                0 & 1\\
            \end{bmatrix}
            \begin{amatrix}{2}
                \sqrt{2} & 0 & 0\\
                0 & 1 & 0\\
            \end{amatrix}
            \renewcommand{\arraystretch}{1.6}
            \begin{bmatrix}
                \frac{1}{\sqrt{2}} & 0 & \frac{-1}{\sqrt{2}}\\
                \frac{1}{\sqrt{2}} & 0 & \frac{1}{\sqrt{2}}\\
                0 & 1 & 0\\
            \end{bmatrix}^\T
        $
    \end{itemize}
\end{itemize}


\subsection*{Matrix Approximations / Correlations}
\begin{itemize}
    \item \marginnote{3/10:}The importance of SVD:
    \begin{itemize}
        \item $A=U\Sigma V^\T$
        \begin{equation*}
            \begin{bmatrix}
                {\color{red!50!blue}|} & & {\color{red}|} & & |\\
                {\color{red!50!blue}u_1} & {\color{red!50!blue}\cdots} & {\color{red}u_m} & \cdots & u_n\\
                {\color{red!50!blue}|} & & {\color{red}|} & & |\\
            \end{bmatrix}
            \begin{bmatrix}
                {\color{red!50!blue}\sigma_1} & & &\\
                & {\color{red!50!blue}\ddots} & & 0\\
                & & {\color{red}\sigma_m} &\\
                & 0 & & 0\\ 
            \end{bmatrix}
            \begin{bmatrix}
                {\color{red!50!blue}\text{---}} & {\color{red!50!blue}v_1^\T} & {\color{red!50!blue}\text{---}}\\
                & {\color{red!50!blue}\vdots} & \\
                {\color{red}\text{---}} & {\color{red}v_m^\T} & {\color{red}\text{---}}\\
            \end{bmatrix}
        \end{equation*}
        \item $\sigma_1\geq\sigma_2\geq\cdots\geq\sigma_m$, so the principal eigenvector corresponds to the largest singular value, and then on and on down the list.
        \begin{align*}
            A &= \sigma_1u_1v_1^\T+\sigma_2u_2v_2^\T+\cdots+\sigma_mu_mv_m^\T+0+\cdots+0\\
            &= \sigma_1
            \begin{bmatrix}
                |\\
                u_1\\
                |\\
            \end{bmatrix}
            \begin{bmatrix}
                \text{---} & v_1^\T & \text{---}
            \end{bmatrix}
            +
            \sigma_2
            \begin{bmatrix}
                |\\
                u_2\\
                |\\
            \end{bmatrix}
            \begin{bmatrix}
                \text{---} & v_2^\T & \text{---}
            \end{bmatrix}
            +\cdots
        \end{align*}
        \item From a data science perspective:
        \item Each term above is a rank 1 matrix.
        \item The SVD is nothing more than a sum of rank 1 matrices.
        \item We order it $\sigma_1\geq\sigma_2\geq\cdots\geq\sigma_m$ because this way, we add on less and less the farther along the sum we go. In other words, we can slowly build an approximation of $A$ from rank 1 matrices via a sum. The terms at the end are not of any real importance. You can truncate the sum at a certain point, yielding an approximation from just a couple of sums.
        \item From an SVD, you can see the most important and strongest building blocks of $A$.
        \item This approximation is denoted $A= \hat{U}\hat{\Sigma}V^\T$ --- an \textcolor{red}{economy SVD}.
        \item \textbf{Economy SVD}s ignore the extra eigenvectors in $U$.
        \begin{itemize}
            \item First $m$ columns of $U$.
            \item First $m$ singular values.
            \item All of $V^\T$
            \item Not an approximation (all other eigenvectors in $u$ are multiplied by 0s in $\Sigma$ initially).
        \end{itemize}
        \item To \textcolor{blue}{approximate}, let $A\approx \tilde{U}\tilde{\Sigma}\tilde{V}^\T$.
        \begin{itemize}
            \item Choose $r$ singular values based upon dominance.
        \end{itemize}
    \end{itemize}
    \item Consider the correlation between the vectors of $A$:
    \begin{itemize}
        \item $A$ is tall and skinny.
        \item Let $A = U\Sigma V^\T$.
        \begin{align*}
            A^\T A &= V\Sigma U^\T U\Sigma V^\T\\
            &= V\Sigma^2V^\T\\
            A^\T A &= V\Lambda V^\T\\
            A^\T AV &= V\Lambda
        \end{align*}
        \item $
            A^\T A =
            \begin{bmatrix}
                \text{---} & a_1^\T & \text{---}\\
                 & \vdots & \\
                \text{---} & a_m^\T & \text{---}\\
            \end{bmatrix}
            \begin{bmatrix}
                | &  & |\\
                a_1 & \cdots & a_m\\
                | &  & |\\
            \end{bmatrix}
            =
            \begin{bmatrix}
                a_1^\T a_1 & \cdots & a_1^\T a_m\\
                \vdots & \ddots & \\
                a_m^\T a_1 &  & a_m^\T a_m\\
            \end{bmatrix}
        $
        \item $u^\T v = \cos(\theta)\cdot||u||\cdot||v||$
        \item The larger the dot product, the greater the correlation.
        \begin{figure}[H]
            \centering
            \begin{subfigure}[b]{0.3\linewidth}
                \centering
                \begin{tikzpicture}
                    \draw (-2,0) -- (2,0);
                    \draw (0,-2) -- (0,2);

                    \draw [thick,->] (0,0) -- (1.6,1.8);
                    \draw [thick,->] (0,0) -- (1.6,1.6);
                \end{tikzpicture}
            \end{subfigure}
            \begin{subfigure}[b]{0.3\linewidth}
                \centering
                \begin{tikzpicture}
                    \draw (-2,0) -- (2,0);
                    \draw (0,-2) -- (0,2);

                    \draw [thick,->] (0,0) -- (1.6,1.8);
                    \draw [thick,->] (0,0) -- (-0.7,-0.4);
                \end{tikzpicture}
            \end{subfigure}\begin{subfigure}[b]{0.3\linewidth}
                \centering
                \begin{tikzpicture}[
                    point/.style={circle,fill,inner sep=1.5pt}
                ]
                    \draw (-2,0) -- (2,0);
                    \draw (0,-2) -- (0,2);

                    \node [point,label={below:$0$},label={above right:$(1,0)$}] at (0:1.5) {};
                    \node [point,label={left:$\frac{\pi}{2}$},label={right:$(0,1)$}] at (90:1.5) {};
                    \node [point,label={above left:$\pi$},label={above right:$(-1,0)$}] at (180:1.5) {};
                \end{tikzpicture}
            \end{subfigure}
        \end{figure}
        \item $U$ tells us how related all vectors in $A$ are.
        \item $V$ tells us how unrelated all vectors in $A$ are.
    \end{itemize}
    \item Consider a real world example. Let's imagine Netflix users 1-7 have ranked 5 films on a 0-5 scale, producing the following matrix.
    \begin{center}
        \begin{tikzpicture}[
            node distance=1.5mm,
            every left delimiter/.style={xshift=2.5mm},
            every right delimiter/.style={xshift=-2.5mm}
        ]
            \matrix (M) [
                matrix of math nodes,
                left delimiter={[},right delimiter={]},
                outer ysep=-1mm
            ]
            {
                1 & 1 & 1 & 0 & 0\\
                3 & 3 & 3 & 0 & 0\\
                4 & 4 & 4 & 0 & 0\\
                5 & 5 & 5 & 0 & 0\\
                0 & 2 & 0 & 4 & 4\\
                0 & 0 & 0 & 5 & 5\\
                0 & 1 & 0 & 2 & 2\\
            };

            \node [left=of M,xshift=2mm] {$M =$};
            \node [above=of M-1-1,anchor=west,rotate=90] {LOTR};
            \node [above=of M-1-2,anchor=west,rotate=90] {HP};
            \node [above=of M-1-3,anchor=west,rotate=90] {Hobbit};
            \node [above=of M-1-4,anchor=west,rotate=90] {Spiderman};
            \node [above=of M-1-5,anchor=west,rotate=90] {Avengers};
            \foreach \row in {1,...,7} {
                \node [right=of M-\row-5,anchor=west] {User \row};
            }
        \end{tikzpicture}
    \end{center}
    \begin{itemize}
        \item The matrices $U$, $\Sigma$, and $V$ for the SVD of $M$ can be found to be approximately the following.
    \end{itemize}
    \begin{align*}
        U &=
        \begin{bmatrix}
            \color{red} 0.137599 & -0.0236115 & 0.0108085 & 0 & -0.980591 & -0.121046 & -0.0649204\\
            \color{red} 0.412696 & -0.0708344 & 0.0324254 & 0 & 0 & 0 & 0.907495\\
            \color{red} 0.550397 & -0.0944458 & 0.0432339 & 0 & 0 & 0.786796 & -0.259281\\
            \color{red} 0.687996 & -0.118057 & 0.0540424 & 0 & 0.196116 & -0.605228 & -0.324102\\
            0.152775 & \color{blue} 0.591101 & -0.653651 & -0.447214 & 0 & 0 & 0\\
            0.0722165 & \color{blue}0.731312 & 0.678209 & 0 & 0 & 0 & 0\\
            0.763875 & \color{blue}0.29555 & -0.326825 & 0.894427 & 0 & 0 & 0\\
        \end{bmatrix}\\
        \Sigma &=
        \begin{bmatrix}
            \color{red} 12.481 & 0 & 0 & 0 & 0\\
            0 & \color{blue} 9.50861 & 0 & 0 & 0\\
            0 & 0 & 1.34556 & 0 & 0\\
            0 & 0 & 0 & 0 & 0\\
            0 & 0 & 0 & 0 & 0\\
            0 & 0 & 0 & 0 & 0\\
            0 & 0 & 0 & 0 & 0\\
        \end{bmatrix}\\
        V &=
        \begin{bmatrix}
            \color{red} 0.562258 & -0.126641 & 0.40967 & 0 & -0.707107\\
            \color{red} 0.59286 & 0.0287706 & -0.804792 & 0 & 0\\
            \color{red} 0.562258 & -0.126641 & 0.409667 & 0 & 0.707107\\
            0.0901335 & \color{blue} 0.695376 & 0.0912571 & -0.707107 & 0\\
            0.0901335 & \color{blue} 0.695376 & 0.0912571 & 0.707107 & 0\\
        \end{bmatrix}
    \end{align*}
    \begin{itemize}
        \item The \textcolor{red}{red} entries in $U$ show that users 1-4 liked the fantasy genre (Lord of the Rings, Harry Potter, and The Hobbit) more. On the other hand, the \textcolor{blue}{blue} entries in $U$ show that users 5-7 liked the superhero genre (Spiderman and The Avengers) more.
        \begin{itemize}
            \item $U$ correlates users with genres.
        \end{itemize}
        \item The \textcolor{red}{red} entries in $V$ show that LOTR, HP, and Hobbit represent the fantasy genre better than the others, with HP being the most well received representative. On the other hand, the \textcolor{blue}{blue} entries in $V$ show that Spiderman and Avengers represent the superhero genre better than the others; both do so equally well.
        \begin{itemize}
            \item $V$ correlates films with genres.
        \end{itemize}
        \item The \textcolor{red}{red} entry in $\Sigma$ represents the fantasy genre (which is liked the most). The \textcolor{blue}{blue} entry in $\Sigma$ represents the superhero genre (which is still well liked). The last nonzero entry in $\Sigma$ is low enough, comparatively, to be insignificant, and is just a meaningless leftover of the SVD process.
    \end{itemize}
\end{itemize}




\end{document}