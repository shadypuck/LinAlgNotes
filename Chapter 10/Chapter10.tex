\documentclass{article}

\input{../preamble.tex}

\begin{document}




\lhead{Chapter 10: Complex Vectors and Matrices}
\section*{Complex Linear Independence: Decomplexification}
\begin{itemize}
    \item \marginnote{4/7:}When given a complex system of equations, it is necessary to \textbf{decomplexify} it.
    \item \textbf{Decomplexify}: To model a complex system of equations with a strictly real system for the purpose of applying the tenets of real linear algebra to it.
    \item Consider the following complex system of equations.
    \begin{align*}
        (2+i)x_1+(1+i)x_2 &= 3+6i\\
        (3-i)x_1+(2-2i)x_2 &= 7-i
    \end{align*}
    \begin{itemize}
        \item The solutions will be complex numbers: $x_1=a_1+ib_1$ and $x_2=a_2+ib_2$, where $a_1,a_2,b_1,b_2\in\R$.
    \end{itemize}
    \item Transform it into a matrix system of equations. Separate the real and complex parts, and factor out all instances of the imaginary number $i$ so that it is a coefficient to any complex matrix.
    \begin{align*}
        \begin{bmatrix}
            2+i & 1+i\\
            3-i & 2-2i\\
        \end{bmatrix}
        \begin{bmatrix}
            a_1+ib_1\\
            a_2+ib_2\\
        \end{bmatrix}
        &=
        \begin{bmatrix}
            3+6i\\
            7-i\\
        \end{bmatrix}\\
        \left(
            \begin{bmatrix}
                2 & 1\\
                3 & 2\\
            \end{bmatrix}
            +
            \begin{bmatrix}
                i & i\\
                -i & -2i\\
            \end{bmatrix}
        \right)\left(
            \begin{bmatrix}
                a_1\\
                a_2\\
            \end{bmatrix}
            +
            \begin{bmatrix}
                ib_1\\
                ib_2\\
            \end{bmatrix}
        \right) &= \left(
            \begin{bmatrix}
                3\\
                7\\
            \end{bmatrix}
            +
            \begin{bmatrix}
                6i\\
                -i\\
            \end{bmatrix}
        \right)\\
        \underbrace{\left(
            \begin{bmatrix}
                2 & 1\\
                3 & 2\\
            \end{bmatrix}
            +i
            \begin{bmatrix}
                1 & 1\\
                -1 & -2\\
            \end{bmatrix}
        \right)}_A \underbrace{\left(
            \begin{bmatrix}
                a_1\\
                a_2\\
            \end{bmatrix}
            +i
            \begin{bmatrix}
                b_1\\
                b_2\\
            \end{bmatrix}
        \right)}_x &= \underbrace{\left(
            \begin{bmatrix}
                3\\
                7\\
            \end{bmatrix}
            +i
            \begin{bmatrix}
                6\\
                -1\\
            \end{bmatrix}
        \right)}_b
    \end{align*}
    \item Foil the left side of the above equation\footnote{Note that the minus sign appears in the real component because, when multiplying the two "last" parts, $i^2=-1$.}.
    \begin{equation*}
        \left(
            \begin{bmatrix}
                2 & 1\\
                3 & 2\\
            \end{bmatrix}
            \begin{bmatrix}
                a_1\\
                a_2\\
            \end{bmatrix}
            -
            \begin{bmatrix}
                1 & 1\\
                -1 & -2\\
            \end{bmatrix}
            \begin{bmatrix}
                b_1\\
                b_2\\
            \end{bmatrix}
        \right)+i\left(
            \begin{bmatrix}
                2 & 1\\
                3 & 2\\
            \end{bmatrix}
            \begin{bmatrix}
                b_1\\
                b_2\\
            \end{bmatrix}
            +
            \begin{bmatrix}
                1 & 1\\
                -1 & -2\\
            \end{bmatrix}
            \begin{bmatrix}
                a_1\\
                a_2\\
            \end{bmatrix}
        \right) =
        \begin{bmatrix}
            3\\
            7\\
        \end{bmatrix}
        +i
        \begin{bmatrix}
            6\\
            -1\\
        \end{bmatrix}
    \end{equation*}
    \item Split the above system of equations into a real system of equations and a complex system of equations by setting equal to each other the real components of each side and the imaginary components of each side.
    \begin{align*}
        \begin{bmatrix}
            2 & 1\\
            3 & 2\\
        \end{bmatrix}
        \begin{bmatrix}
            a_1\\
            a_2\\
        \end{bmatrix}
        -
        \begin{bmatrix}
            1 & 1\\
            -1 & -2\\
        \end{bmatrix}
        \begin{bmatrix}
            b_1\\
            b_2\\
        \end{bmatrix}
        &=
        \begin{bmatrix}
            3\\
            7\\
        \end{bmatrix}\\
        \begin{bmatrix}
            2 & 1\\
            3 & 2\\
        \end{bmatrix}
        \begin{bmatrix}
            b_1\\
            b_2\\
        \end{bmatrix}
        +
        \begin{bmatrix}
            1 & 1\\
            -1 & -2\\
        \end{bmatrix}
        \begin{bmatrix}
            a_1\\
            a_2\\
        \end{bmatrix}
        &=
        \begin{bmatrix}
            6\\
            -1\\
        \end{bmatrix}
    \end{align*}
    \item Multiply out the matrices above to yield a system of four equations.
    \begin{align*}
        2a_1+a_2-b_1-b_2 &= 3\\
        3a_1+2a_2+b_1+2b_2 &= 7\\
        a_1+a_2+2b_1+b_2 &= 6\\
        -a_1-2a_2+3b_1+2b_2 &= -1
    \end{align*}
    \item Condense the above system of equations into a single matrix system of equations.
    \begin{align*}
        \begin{bmatrix}
            2 & 1 & -1 & -1\\
            3 & 2 & 1 & 2\\
            1 & 1 & 2 & 1\\
            -1 & -2 & 3 & 2\\
        \end{bmatrix}
        \begin{bmatrix}
            a_1\\
            a_2\\
            b_1\\
            b_2\\
        \end{bmatrix}
        =
        \begin{bmatrix}
            3\\
            7\\
            6\\
            -1\\
        \end{bmatrix}
    \end{align*}
    \item Solve for $a_1$, $a_2$, $b_1$, and $b_2$ using an augmented matrix and Gauss-Jordan elimination.
    \begin{equation*}
        \begin{amatrix}{4}
            2 & 1 & -1 & -1 & 3\\
            3 & 2 & 1 & 2 & 7\\
            1 & 1 & 2 & 1 & 6\\
            -1 & -2 & 3 & 2 & -1\\
        \end{amatrix}
        \rightarrow
        \begin{amatrix}{4}
            1 & 0 & 0 & 0 & 1\\
            0 & 1 & 0 & 0 & 2\\
            0 & 0 & 1 & 0 & 2\\
            0 & 0 & 0 & 1 & -1\\
        \end{amatrix}
    \end{equation*}
    \begin{equation*}
        \begin{bmatrix}
            a_1\\
            a_2\\
            b_1\\
            b_2\\
        \end{bmatrix}
        =
        \begin{bmatrix}
            1\\
            2\\
            2\\
            -1\\
        \end{bmatrix}
    \end{equation*}
    \item From these four values, the original solutions $x_1=a_1+ib_1$ and $x_2=a_2+ib_2$ can be found.
    \begin{align*}
        x_1 &= 1+2i\\
        x_2 &= 2-i
    \end{align*}
\end{itemize}



\section*{Hermitian, Unitary, and Normal Matrices}
\begin{itemize}
    \item \marginnote{4/13:}What necessitates different categorizations of complex vectors and matrices?
    \item Consider a vector $v$.
    \begin{equation*}
        v =
        \begin{bmatrix}
            1\\
            i\\
        \end{bmatrix}
    \end{equation*}
    \item If you want to find $||v||$, you typically evaluate $\sqrt{v^\T v}$. However, this equals to 0 (see the following), which is clearly not the magnitude of $v$.
    \begin{equation*}
        \begin{bmatrix}
            1 & i\\
        \end{bmatrix}
        \begin{bmatrix}
            1\\
            i\\
        \end{bmatrix}
        = 1-1 = 0
    \end{equation*}
    \begin{itemize}
        \item Note that $||v||$ must be an element of $\R$ because it measures a distance.
    \end{itemize}
    \item With complex vectors, it is necessary to evaluate $\sqrt{\bar{v}^\T v}$ to find $||v||$.
    \begin{equation*}
        \begin{bmatrix}
            1 & -i\\
        \end{bmatrix}
        \begin{bmatrix}
            1\\
            i\\
        \end{bmatrix}
        = 1+1 = 2
    \end{equation*}
    \begin{equation*}
        ||v|| = \sqrt{2}
    \end{equation*}
    \begin{itemize}
        \item This makes sense because $
            \begin{bmatrix}
                1\\
                i\\
            \end{bmatrix}
        $ extends one unit into $\R^1$ and one unit into $\C^2$.
        \item If $z\bar{z}=|z|^2$ and $\bar{v}^\T v=v\cdot \bar{v}$, it stands to reason that $\bar{v}^\T v=||v||^2$. Essentially, the dot product multiplies every element of $v$ by its complex conjugate and sums them.
    \end{itemize}
    \item Instead of writing $\bar{v}^\T$\footnote{"$v$ conjugate transpose"} every time, mathematicians shorthand to $v^\He$\footnote{"$v$ Hermitian" after French mathematician Charles Hermite.}.
    \begin{itemize}
        \item $v^\He$ works for all vectors, but it is necessary for complex ones.
    \end{itemize}
    \item \textbf{Hermitian} (matrix): A matrix $A$ such that $A=A^\He$.
    \begin{itemize}
        \item Typically defined for $A\in\C^n$, but holds for $A\in\R^n$, too.
        \item Parallel to how if $A\in\R^n$ and $A=A^\T$, $A$ is symmetrical.
        \item Also note that if $A^\He A=A^2=AA^\He$, $A$ is Hermitian.
    \end{itemize}
    \item \textbf{Unitary} (matrix): A matrix $A$ such that $A^{-1}=A^\He$.
    \begin{itemize}
        \item Typically defined for $A\in\C^n$, but holds for $A\in\R^n$, too.
        \item Parallel to how if $A\in\R^n$ and $A^{-1}=A^\T$, $A$ is orthonormal.
        \item Also note that if $A^\He A=I=AA^\He$, $A$ is unitary.
    \end{itemize}
    \item \textbf{Normal} (matrix): A matrix that is unitarily diagonalizable.
    \begin{itemize}
        \item Typically defined for $A\in\C^n$, but holds for $A\in\R^n$, too.
        \item Parallel to matrices $A\in\R^n$ such that $A$ is orthonormally diagonalizable.
    \end{itemize}
    \item Note that not every complex matrix has to be one of these three types.
    \item When $A^\He A=AA^\He$, $A=U\Lambda U^\He$.
    \begin{align*}
        AA^\He &= \left( U\Lambda U^\He \right)\left( U\Lambda U^\He \right)^\He\\
        &= U\Lambda U^\He U\Lambda^\He U^\He\\
        &= U\Lambda\Lambda^\He U^\He\\
        &= U\Lambda^\He\Lambda U^\He\footnotemark\\
        &= U\Lambda^\He U^\He U\Lambda U^\He\\
        &= \left( U\Lambda U^\He \right)^\He\left( U\Lambda U^\He \right)\\
        &= A^\He A
    \end{align*}
    \footnotetext{Since $\Lambda=\Lambda^\He$.}
    \item When $A=A^\He$, all eigenvalues are elements of $\R$ (similar to spectral theorem).
    \begin{equation*}
        v^\He Av = \left( v^\He Av \right)^\He = v^\He Av
    \end{equation*}
    \begin{itemize}
        \item The above proves that $v^\He Av\in\R$ because it's its own conjugate\footnote{Recall that only real quantities can be their own conjugates because $a+0i=a-0i$.}.
        \begin{align*}
            Av &= \lambda v\\
            v^\He Av &= \lambda v^\He v
        \end{align*}
        \item $\lambda = \frac{v^\He Av}{v^\He v} \rightarrow \frac{\R}{\R} = \R$\footnote{Note that the denominator is real because it's how one finds $||v||$, and $||v||$ must be real, as discussed above.}.
    \end{itemize}
    \item When $A=A^\He$ and $Ax=\lambda x$, all $x$'s can be chosen orthonormally (also similar to spectral theorem).
    \begin{itemize}
        \item Normality is implied because any eigenvector can be scaled to any version (including a normal version) and still be an eigenvector.
        \begin{align*}
            x_i &=
            \begin{bmatrix}
                x_{i_1}\\
                x_{i_2}\\
                \vdots\\
                x_{i_n}\\
            \end{bmatrix}&
                x_i^\He &=
                \begin{bmatrix}
                    \bar{x}_{i_1} & \bar{x}_{i_2} & \cdots & \bar{x}_{i_n}\\
                \end{bmatrix}
        \end{align*}
        \begin{align*}
            A &=
            \begin{bmatrix}
                a_1 & a_2\\
                a_3 & a_4\\
            \end{bmatrix}&
                A^\He &=
                \begin{bmatrix}
                    a_1 & \bar{a}_3\\
                    \bar{a}_2 & a_4\\
                \end{bmatrix}
        \end{align*}
        \item Define an arbitrary vector $x_i$ and matrix $A$, along with their conjugate transposes (or Hermitian versions).
        \begin{align*}
            Ax_1 &= \lambda_1x_1\\
            {\color{blue}x_2^\He Ax_1} &= {\color{red}\lambda_1x_2^\He x_1}
        \end{align*}
        \begin{align*}
            Ax_2 &= \lambda_2x_2\\
            (Ax_2)^\He &= (\lambda_2x_2)^\He\\
            x_2^\He A^\He &= \lambda_2x_2^\He\\
            {\color{blue}x_2^\He Ax_1} &= {\color{grx}\lambda_2x_2^\He x_1}
        \end{align*}
        \item ${\color{red}\lambda_1x_2^\He x_1} = {\color{grx}\lambda_2x_2^\He x_1}$ implies that, since $\lambda_1\neq\lambda_2$, $x_2^\He x_1$ must equal 0, proving orthogonality.
    \end{itemize}
\end{itemize}




\end{document}